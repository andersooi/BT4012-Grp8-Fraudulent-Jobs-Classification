{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d069ad8",
   "metadata": {},
   "source": [
    "### E2E ML Pipeline\n",
    "\n",
    "This notebook details the feature engineering pipeline as well as the model exploration and evaluation process. At the end, we full trained the Bi-directional LSTM and DistilBERT models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeef4a2f",
   "metadata": {},
   "source": [
    "Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439c3bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, LSTM, Bidirectional, Concatenate, TextVectorization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, AdamW\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6d661a",
   "metadata": {},
   "source": [
    "Loading of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59da53aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>department</th>\n",
       "      <th>salary_range</th>\n",
       "      <th>company_profile</th>\n",
       "      <th>description</th>\n",
       "      <th>requirements</th>\n",
       "      <th>benefits</th>\n",
       "      <th>telecommuting</th>\n",
       "      <th>has_company_logo</th>\n",
       "      <th>has_questions</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>required_experience</th>\n",
       "      <th>required_education</th>\n",
       "      <th>industry</th>\n",
       "      <th>function</th>\n",
       "      <th>fraudulent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Marketing Intern</td>\n",
       "      <td>US, NY, New York</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We're Food52, and we've created a groundbreaki...</td>\n",
       "      <td>Food52, a fast-growing, James Beard Award-winn...</td>\n",
       "      <td>Experience with content management systems a m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "      <td>Internship</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Customer Service - Cloud Video Production</td>\n",
       "      <td>NZ, , Auckland</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90 Seconds, the worlds Cloud Video Production ...</td>\n",
       "      <td>Organised - Focused - Vibrant - Awesome!Do you...</td>\n",
       "      <td>What we expect from you:Your key responsibilit...</td>\n",
       "      <td>What you will get from usThrough being part of...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Marketing and Advertising</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Commissioning Machinery Assistant (CMA)</td>\n",
       "      <td>US, IA, Wever</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Valor Services provides Workforce Solutions th...</td>\n",
       "      <td>Our client, located in Houston, is actively se...</td>\n",
       "      <td>Implement pre-commissioning and commissioning ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Account Executive - Washington DC</td>\n",
       "      <td>US, DC, Washington</td>\n",
       "      <td>Sales</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our passion for improving quality of life thro...</td>\n",
       "      <td>THE COMPANY: ESRI – Environmental Systems Rese...</td>\n",
       "      <td>EDUCATION: Bachelor’s or Master’s in GIS, busi...</td>\n",
       "      <td>Our culture is anything but corporate—we have ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>Computer Software</td>\n",
       "      <td>Sales</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Bill Review Manager</td>\n",
       "      <td>US, FL, Fort Worth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SpotSource Solutions LLC is a Global Human Cap...</td>\n",
       "      <td>JOB TITLE: Itemization Review ManagerLOCATION:...</td>\n",
       "      <td>QUALIFICATIONS:RN license in the State of Texa...</td>\n",
       "      <td>Full Benefits Offered</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>Hospital &amp; Health Care</td>\n",
       "      <td>Health Care Provider</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   job_id                                      title            location  \\\n",
       "0       1                           Marketing Intern    US, NY, New York   \n",
       "1       2  Customer Service - Cloud Video Production      NZ, , Auckland   \n",
       "2       3    Commissioning Machinery Assistant (CMA)       US, IA, Wever   \n",
       "3       4          Account Executive - Washington DC  US, DC, Washington   \n",
       "4       5                        Bill Review Manager  US, FL, Fort Worth   \n",
       "\n",
       "  department salary_range                                    company_profile  \\\n",
       "0  Marketing          NaN  We're Food52, and we've created a groundbreaki...   \n",
       "1    Success          NaN  90 Seconds, the worlds Cloud Video Production ...   \n",
       "2        NaN          NaN  Valor Services provides Workforce Solutions th...   \n",
       "3      Sales          NaN  Our passion for improving quality of life thro...   \n",
       "4        NaN          NaN  SpotSource Solutions LLC is a Global Human Cap...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Food52, a fast-growing, James Beard Award-winn...   \n",
       "1  Organised - Focused - Vibrant - Awesome!Do you...   \n",
       "2  Our client, located in Houston, is actively se...   \n",
       "3  THE COMPANY: ESRI – Environmental Systems Rese...   \n",
       "4  JOB TITLE: Itemization Review ManagerLOCATION:...   \n",
       "\n",
       "                                        requirements  \\\n",
       "0  Experience with content management systems a m...   \n",
       "1  What we expect from you:Your key responsibilit...   \n",
       "2  Implement pre-commissioning and commissioning ...   \n",
       "3  EDUCATION: Bachelor’s or Master’s in GIS, busi...   \n",
       "4  QUALIFICATIONS:RN license in the State of Texa...   \n",
       "\n",
       "                                            benefits  telecommuting  \\\n",
       "0                                                NaN              0   \n",
       "1  What you will get from usThrough being part of...              0   \n",
       "2                                                NaN              0   \n",
       "3  Our culture is anything but corporate—we have ...              0   \n",
       "4                              Full Benefits Offered              0   \n",
       "\n",
       "   has_company_logo  has_questions employment_type required_experience  \\\n",
       "0                 1              0           Other          Internship   \n",
       "1                 1              0       Full-time      Not Applicable   \n",
       "2                 1              0             NaN                 NaN   \n",
       "3                 1              0       Full-time    Mid-Senior level   \n",
       "4                 1              1       Full-time    Mid-Senior level   \n",
       "\n",
       "  required_education                   industry              function  \\\n",
       "0                NaN                        NaN             Marketing   \n",
       "1                NaN  Marketing and Advertising      Customer Service   \n",
       "2                NaN                        NaN                   NaN   \n",
       "3  Bachelor's Degree          Computer Software                 Sales   \n",
       "4  Bachelor's Degree     Hospital & Health Care  Health Care Provider   \n",
       "\n",
       "   fraudulent  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/fake_job_postings.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b65dec9",
   "metadata": {},
   "source": [
    "### Feature Engineering Pipeline\n",
    "\n",
    "In the next section we will create reusable functions that can be fit into a pipeline. We start with creating features from numeric and categorical columns from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af55a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCAMMY_PHRASES = [\n",
    "    'urgent', 'guaranteed', 'money', 'cash', 'quick cash', 'investment', 'upfront fee', 'wire transfer',\n",
    "    'limited time', 'winner', 'prize', 'bonus', 'earn', 'easy money', 'no experience', 'click here',\n",
    "    'apply fast', 'instant', 'payable immediately', 'work from home', 'sms us', 'act now', 'free',\n",
    "    'risk free', 'no risk', 'make money fast', 'earn extra income', 'financial freedom', 'be your own boss',\n",
    "    'unlimited earning', 'passive income', 'get paid', 'weekly pay', 'daily pay', 'commission',\n",
    "    'multi level marketing', 'mlm', 'pyramid', 'recruitment', 'sign up fee', 'registration fee',\n",
    "    'processing fee', 'training fee', 'starter kit', 'pay to apply', 'credit card required',\n",
    "    'bank account', 'personal information', 'social security', 'copy and paste', 'no selling',\n",
    "    'no interview', 'hired immediately', 'start today', 'start immediately', 'too good to be true'\n",
    "]\n",
    "\n",
    "UNIVERSAL_CURRENCIES = [\n",
    "    '$', '€', '£', '¥', '₹', '₽', '₩', '₪', '₱', '₫', '฿', '₡', '₦', '₨', '₴', '₵', '₸', '₺', '₼', '₾',\n",
    "    'USD', 'EUR', 'GBP', 'JPY', 'CNY', 'INR', 'RUB', 'KRW', 'AUD', 'CAD', 'CHF', 'HKD', 'SGD', 'SEK',\n",
    "    'NOK', 'DKK', 'PLN', 'THB', 'MXN', 'BRL', 'ZAR', 'TRY', 'IDR', 'MYR', 'PHP', 'VND', 'AED', 'SAR',\n",
    "    'ILS', 'EGP', 'NGN', 'PKR', 'BDT', 'UAH', 'CZK', 'HUF', 'RON', 'NZD', 'CLP', 'ARS', 'COP', 'PEN'\n",
    "]\n",
    "\n",
    "SENTENCE_TRANSFORMER_MODEL = None\n",
    "\n",
    "def similarity_function(column_a, column_b):\n",
    "    combined = pd.concat([column_a, column_b], axis=0)\n",
    "    embeddings = embed_texts(combined.tolist())\n",
    "    first = embeddings[: len(column_a)]\n",
    "    second = embeddings[len(column_a) :]\n",
    "    similarities = rowwise_cosine(first, second)\n",
    "    return pd.Series(similarities, index=column_a.index)\n",
    "\n",
    "def embed_texts(texts):\n",
    "    global SENTENCE_TRANSFORMER_MODEL\n",
    "    if SENTENCE_TRANSFORMER_MODEL is None:\n",
    "        import torch\n",
    "        # check if theres gpu\n",
    "        if torch.backends.mps.is_available():\n",
    "            device = 'mps'\n",
    "        elif torch.cuda.is_available():\n",
    "            device = 'cuda'\n",
    "        else:\n",
    "            device = 'cpu'\n",
    "\n",
    "        SENTENCE_TRANSFORMER_MODEL = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "        print(f\"[INFO] Using {device} for sentence transformer\")\n",
    "    return np.asarray(SENTENCE_TRANSFORMER_MODEL.encode(list(texts), show_progress_bar=True, batch_size=128, convert_to_numpy=True))\n",
    "\n",
    "def rowwise_cosine(first, second):\n",
    "    numerators = np.sum(first * second, axis=1)\n",
    "    denom = np.linalg.norm(first, axis=1) * np.linalg.norm(second, axis=1)\n",
    "    denom = np.where(denom == 0, 1e-12, denom)\n",
    "    return numerators / denom\n",
    "\n",
    "def create_textual_features(df):\n",
    "    # create output dataframe\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    # get salary_range\n",
    "    salary_range = df['salary_range']\n",
    "    # create binary column for salary_range\n",
    "    out['has_salary_range'] = salary_range.notna().astype(int)\n",
    "\n",
    "    # for has required experience\n",
    "    required_experience = df['required_experience']\n",
    "    out['has_required_experience'] = required_experience.notna().astype(int)\n",
    "\n",
    "    # for has employment type\n",
    "    employment_type = df['employment_type']\n",
    "    out['has_employment_type'] = employment_type.notna().astype(int)\n",
    "\n",
    "    # for has required education\n",
    "    required_education = df['required_education']\n",
    "    out['has_required_education'] = required_education.notna().astype(int)\n",
    "\n",
    "    # for has company profile\n",
    "    company_profile = df['company_profile']\n",
    "    out['has_company_profile'] = company_profile.notna().astype(int)\n",
    "    \n",
    "    # calculate lengths for description, company_profile, requirements, and benefits\n",
    "    out['description_length'] = df['description'].fillna('').str.len()\n",
    "    out['company_profile_length'] = df['company_profile'].fillna('').str.len()\n",
    "    out['requirements_length'] = df['requirements'].fillna('').str.len()\n",
    "    out['benefits_length'] = df['benefits'].fillna('').str.len()\n",
    "    \n",
    "    # engineered features\n",
    "    # legitimacy index through weighted sum \n",
    "    # description 0.3, requirements 0.4, logo 0.1, profile 0.2\n",
    "    has_company_logo = df['has_company_logo']\n",
    "    has_company_profile = out['has_company_profile']\n",
    "    has_requirements = df['requirements'].notna().astype(int)\n",
    "    has_description = df['description'].notna().astype(int)\n",
    "    out['legitimacy_index'] = 0.3 * has_description + 0.4 * has_requirements + 0.1 * has_company_logo + 0.2 * has_company_profile\n",
    "\n",
    "    # cosine similarities\n",
    "    profile_desc_sim = similarity_function(df['company_profile'].fillna(''), df['description'].fillna(''))\n",
    "    # do normalisation to output 0 to 1\n",
    "    out['profile_desc_similarity'] = ((profile_desc_sim + 1) / 2).clip(0.0, 1.0)\n",
    "\n",
    "    descr_req_sim = similarity_function(df['description'].fillna(''), df['requirements'].fillna(''))\n",
    "    out['desc_req_similarity'] = ((descr_req_sim + 1) / 2).clip(0.0, 1.0)\n",
    "\n",
    "    # check whether description column contains '@' or 5+ consecutive digits\n",
    "    out['desc_contact_flag'] = df['description'].fillna('').apply(lambda x: float(bool('@' in x or re.search(r'\\d{5,}', x) is not None)))\n",
    "    \n",
    "    # scammy word count\n",
    "    out['scammy_word_count'] = df['description'].fillna('').apply(lambda x: sum(1 for phrase in SCAMMY_PHRASES if phrase.lower() in x.lower()))\n",
    "\n",
    "    # check if title has currency symbols\n",
    "    out['has_currency_title'] = df['title'].fillna('').apply(lambda x: float(any(currency in x for currency in UNIVERSAL_CURRENCIES)))\n",
    "\n",
    "    # check if description has currency symbols\n",
    "    out['has_currency_description'] = df['description'].fillna('').apply(lambda x: float(any(currency in x for currency in UNIVERSAL_CURRENCIES)))\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8534e978",
   "metadata": {},
   "source": [
    "Functions to extract features from the textual columns of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e93f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCAMMY_PHRASES = [\n",
    "    'urgent', 'guaranteed', 'money', 'cash', 'quick cash', 'investment', 'upfront fee', 'wire transfer',\n",
    "    'limited time', 'winner', 'prize', 'bonus', 'earn', 'easy money', 'no experience', 'click here',\n",
    "    'apply fast', 'instant', 'payable immediately', 'work from home', 'sms us', 'act now', 'free',\n",
    "    'risk free', 'no risk', 'make money fast', 'earn extra income', 'financial freedom', 'be your own boss',\n",
    "    'unlimited earning', 'passive income', 'get paid', 'weekly pay', 'daily pay', 'commission',\n",
    "    'multi level marketing', 'mlm', 'pyramid', 'recruitment', 'sign up fee', 'registration fee',\n",
    "    'processing fee', 'training fee', 'starter kit', 'pay to apply', 'credit card required',\n",
    "    'bank account', 'personal information', 'social security', 'copy and paste', 'no selling',\n",
    "    'no interview', 'hired immediately', 'start today', 'start immediately', 'too good to be true'\n",
    "]\n",
    "\n",
    "UNIVERSAL_CURRENCIES = [\n",
    "    '$', '€', '£', '¥', '₹', '₽', '₩', '₪', '₱', '₫', '฿', '₡', '₦', '₨', '₴', '₵', '₸', '₺', '₼', '₾',\n",
    "    'USD', 'EUR', 'GBP', 'JPY', 'CNY', 'INR', 'RUB', 'KRW', 'AUD', 'CAD', 'CHF', 'HKD', 'SGD', 'SEK',\n",
    "    'NOK', 'DKK', 'PLN', 'THB', 'MXN', 'BRL', 'ZAR', 'TRY', 'IDR', 'MYR', 'PHP', 'VND', 'AED', 'SAR',\n",
    "    'ILS', 'EGP', 'NGN', 'PKR', 'BDT', 'UAH', 'CZK', 'HUF', 'RON', 'NZD', 'CLP', 'ARS', 'COP', 'PEN'\n",
    "]\n",
    "\n",
    "SENTENCE_TRANSFORMER_MODEL = None\n",
    "\n",
    "def similarity_function(column_a, column_b):\n",
    "    combined = pd.concat([column_a, column_b], axis=0)\n",
    "    embeddings = embed_texts(combined.tolist())\n",
    "    first = embeddings[: len(column_a)]\n",
    "    second = embeddings[len(column_a) :]\n",
    "    similarities = rowwise_cosine(first, second)\n",
    "    return pd.Series(similarities, index=column_a.index)\n",
    "\n",
    "def embed_texts(texts):\n",
    "    global SENTENCE_TRANSFORMER_MODEL\n",
    "    if SENTENCE_TRANSFORMER_MODEL is None:\n",
    "        import torch\n",
    "        # check if theres gpu\n",
    "        if torch.backends.mps.is_available():\n",
    "            device = 'mps'\n",
    "        elif torch.cuda.is_available():\n",
    "            device = 'cuda'\n",
    "        else:\n",
    "            device = 'cpu'\n",
    "\n",
    "        SENTENCE_TRANSFORMER_MODEL = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "        print(f\"[INFO] Using {device} for sentence transformer\")\n",
    "    return np.asarray(SENTENCE_TRANSFORMER_MODEL.encode(list(texts), show_progress_bar=True, batch_size=128, convert_to_numpy=True))\n",
    "\n",
    "def rowwise_cosine(first, second):\n",
    "    numerators = np.sum(first * second, axis=1)\n",
    "    denom = np.linalg.norm(first, axis=1) * np.linalg.norm(second, axis=1)\n",
    "    denom = np.where(denom == 0, 1e-12, denom)\n",
    "    return numerators / denom\n",
    "\n",
    "def create_textual_features(df):\n",
    "    # create output dataframe\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    # get salary_range\n",
    "    salary_range = df['salary_range']\n",
    "    # create binary column for salary_range\n",
    "    out['has_salary_range'] = salary_range.notna().astype(int)\n",
    "\n",
    "    # for has required experience\n",
    "    required_experience = df['required_experience']\n",
    "    out['has_required_experience'] = required_experience.notna().astype(int)\n",
    "\n",
    "    # for has employment type\n",
    "    employment_type = df['employment_type']\n",
    "    out['has_employment_type'] = employment_type.notna().astype(int)\n",
    "\n",
    "    # for has required education\n",
    "    required_education = df['required_education']\n",
    "    out['has_required_education'] = required_education.notna().astype(int)\n",
    "\n",
    "    # for has company profile\n",
    "    company_profile = df['company_profile']\n",
    "    out['has_company_profile'] = company_profile.notna().astype(int)\n",
    "    \n",
    "    # calculate lengths for description, company_profile, requirements, and benefits\n",
    "    out['description_length'] = df['description'].fillna('').str.len()\n",
    "    out['company_profile_length'] = df['company_profile'].fillna('').str.len()\n",
    "    out['requirements_length'] = df['requirements'].fillna('').str.len()\n",
    "    out['benefits_length'] = df['benefits'].fillna('').str.len()\n",
    "    \n",
    "    # engineered features\n",
    "    # legitimacy index through weighted sum \n",
    "    # description 0.3, requirements 0.4, logo 0.1, profile 0.2\n",
    "    has_company_logo = df['has_company_logo']\n",
    "    has_company_profile = out['has_company_profile']\n",
    "    has_requirements = df['requirements'].notna().astype(int)\n",
    "    has_description = df['description'].notna().astype(int)\n",
    "    out['legitimacy_index'] = 0.3 * has_description + 0.4 * has_requirements + 0.1 * has_company_logo + 0.2 * has_company_profile\n",
    "\n",
    "    # cosine similarities\n",
    "    profile_desc_sim = similarity_function(df['company_profile'].fillna(''), df['description'].fillna(''))\n",
    "    # do normalisation to output 0 to 1\n",
    "    out['profile_desc_similarity'] = ((profile_desc_sim + 1) / 2).clip(0.0, 1.0)\n",
    "\n",
    "    descr_req_sim = similarity_function(df['description'].fillna(''), df['requirements'].fillna(''))\n",
    "    out['desc_req_similarity'] = ((descr_req_sim + 1) / 2).clip(0.0, 1.0)\n",
    "\n",
    "    # check whether description column contains '@' or 5+ consecutive digits\n",
    "    out['desc_contact_flag'] = df['description'].fillna('').apply(lambda x: float(bool('@' in x or re.search(r'\\d{5,}', x) is not None)))\n",
    "    \n",
    "    # scammy word count\n",
    "    out['scammy_word_count'] = df['description'].fillna('').apply(lambda x: sum(1 for phrase in SCAMMY_PHRASES if phrase.lower() in x.lower()))\n",
    "\n",
    "    # check if title has currency symbols\n",
    "    out['has_currency_title'] = df['title'].fillna('').apply(lambda x: float(any(currency in x for currency in UNIVERSAL_CURRENCIES)))\n",
    "\n",
    "    # check if description has currency symbols\n",
    "    out['has_currency_description'] = df['description'].fillna('').apply(lambda x: float(any(currency in x for currency in UNIVERSAL_CURRENCIES)))\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b58bb4",
   "metadata": {},
   "source": [
    "The pipeline that integrates all the different functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52850327",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_NGRAM_RANGE = (1, 2)\n",
    "MIN_DF = 3\n",
    "WORD_MAX_FEATURES = 5000\n",
    "\n",
    "def transform_text_word_tfidf(ngram_range=WORD_NGRAM_RANGE, min_df=MIN_DF, max_features=WORD_MAX_FEATURES):\n",
    "    return Pipeline([\n",
    "        # combine text fields\n",
    "        ('combine_text', FunctionTransformer(combine_text_fields)),\n",
    "        # then tf-idf\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            ngram_range=WORD_NGRAM_RANGE,\n",
    "            min_df=MIN_DF,\n",
    "            max_features=WORD_MAX_FEATURES,\n",
    "            sublinear_tf=True,\n",
    "            lowercase=True,\n",
    "            stop_words='english'\n",
    "        )) # don't scale to retain sparsity and semantic meaning\n",
    "    ])\n",
    "\n",
    "# add numeric features from categorical columns\n",
    "def transform_numeric_features():\n",
    "    return Pipeline([\n",
    "        # run add_numeric_features function\n",
    "        ('add_numeric', FunctionTransformer(add_numeric_features)),\n",
    "        # ensure output is a numpy array\n",
    "        ('to_matrix', FunctionTransformer(lambda df: df.values if isinstance(df, pd.DataFrame) else df)),\n",
    "        # then standard scale\n",
    "        ('scale', StandardScaler(with_mean=True)) # binary features benefit from centering\n",
    "    ])\n",
    "\n",
    "# add features from categorical columns and then one hot encode\n",
    "def transform_categorical_features():\n",
    "    return Pipeline([\n",
    "        # use wrapper to avoid data leakage (fits major_countries once on training data)\n",
    "        ('add_categorical', CategoricalFeaturesWrapper()),\n",
    "        # then one hot encode\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # no need to scale since they are already on consistent scale of 0/1\n",
    "    ])\n",
    "\n",
    "# add features from textual columns\n",
    "def transform_textual_features():\n",
    "    return Pipeline([\n",
    "        # run create_textual_features function\n",
    "        ('create_textual', FunctionTransformer(create_textual_features)),\n",
    "        # ensure output is a numpy array\n",
    "        ('to_matrix', FunctionTransformer(lambda df: df.values if isinstance(df, pd.DataFrame) else df)),\n",
    "        # then standard scale\n",
    "        ('scale', StandardScaler(with_mean=True)) # mixed feature types benefit from centering\n",
    "    ])\n",
    "\n",
    "# union pipeline\n",
    "def build_complete_pipeline():\n",
    "    return FeatureUnion([\n",
    "        ('numeric', transform_numeric_features()),\n",
    "        ('categorical', transform_categorical_features()),\n",
    "        ('textual', transform_textual_features()),\n",
    "        ('text_word_tfidf', transform_text_word_tfidf())\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3aa86",
   "metadata": {},
   "source": [
    "### Model Evaluation - Stratified K-Fold Cross Validation (CV)\n",
    "\n",
    "In this section, we experiment with traditional ML models (Logistic Regression as the baseline, Gaussian Naive Bayes, Random Forests, XGBoost) and deep learning models (Bi-directional LSTM, DistilBERT) by carrying out CV to have an understanding of how the models will likely perform. SVMSMOTE is also applied on the training fold for oversampling of minority class.\n",
    "\n",
    "*All the models will go through the same pipeline and go through 5 folds.\n",
    "\n",
    "_note: deep learning models go through a separate process as outlined in the report_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d8cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare X and y from the full dataset\n",
    "X = df.drop('fraudulent', axis=1)\n",
    "y = df['fraudulent']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98098b5",
   "metadata": {},
   "source": [
    "_Logistic Regression_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b08789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_lr_oof_metrics(X, y, n_splits=5):\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    oof_lr = np.zeros(len(y), dtype=float)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # build features per fold (your existing pipeline)\n",
    "        features = build_complete_pipeline()\n",
    "        X_train = features.fit_transform(X_train)\n",
    "        X_val = features.transform(X_val)\n",
    "\n",
    "        # Oversample the minority class on the TRAIN data\n",
    "        smote = SVMSMOTE(random_state=42)\n",
    "        X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # ----- Logistic Regression -----\n",
    "        lr = LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        lr.fit(X_res, y_res)\n",
    "\n",
    "        # Out-of-fold predictions (probabilities for class 1)\n",
    "        oof_lr[val_idx] = lr.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        # Hard predictions for metrics\n",
    "        y_pred = lr.predict(X_val)\n",
    "\n",
    "        fold_precision = precision_score(y_val, y_pred)\n",
    "        fold_recall = recall_score(y_val, y_pred)\n",
    "        fold_f1 = f1_score(y_val, y_pred)\n",
    "        fold_auc = roc_auc_score(y_val, oof_lr[val_idx])\n",
    "\n",
    "        print(\n",
    "            f\"[LR] Fold {fold}: \"\n",
    "            f\"Precision={fold_precision:.5f}, \"\n",
    "            f\"Recall={fold_recall:.5f}, \"\n",
    "            f\"F1={fold_f1:.5f}, \"\n",
    "            f\"AUC={fold_auc:.5f}\"\n",
    "        )\n",
    "\n",
    "    # Overall metrics across all folds\n",
    "    y_pred_overall = (oof_lr >= 0.5).astype(int)\n",
    "    overall_precision = precision_score(y, y_pred_overall)\n",
    "    overall_recall = recall_score(y, y_pred_overall)\n",
    "    overall_f1 = f1_score(y, y_pred_overall)\n",
    "    overall_auc = roc_auc_score(y, oof_lr)\n",
    "\n",
    "    return overall_precision, overall_recall, overall_f1, overall_auc, oof_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3de533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 5-fold CV on logistic regr\n",
    "precision, recall, f1, auc, oof_predictions = cv_lr_oof_metrics(X, y, n_splits=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Overall LR Performance (5-Fold CV):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Precision: {precision:.5f}\")\n",
    "print(f\"Recall:    {recall:.5f}\")\n",
    "print(f\"F1 Score:  {f1:.5f}\")\n",
    "print(f\"ROC AUC:   {auc:.5f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd7ddd0",
   "metadata": {},
   "source": [
    "_Gaussian Naive Bayes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e42657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_nb_oof_metrics(X, y, n_splits=5):\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    oof_nb = np.zeros(len(y), dtype=float)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # build features per fold\n",
    "        features = build_complete_pipeline()\n",
    "        X_train = features.fit_transform(X_train)\n",
    "        X_val = features.transform(X_val)\n",
    "\n",
    "        # convert sparse to dense only for Naive Bayes\n",
    "        if hasattr(X_train, 'toarray'):\n",
    "            X_train = X_train.toarray()\n",
    "        if hasattr(X_val, 'toarray'):\n",
    "            X_val = X_val.toarray()\n",
    "\n",
    "        # apply SVMSMOTE only on training fold\n",
    "        smote = SVMSMOTE(random_state=42)\n",
    "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # init naive bayes\n",
    "        model = GaussianNB()\n",
    "        model.fit(X_train, y_train)\n",
    "        oof_nb[val_idx] = model.predict_proba(X_val)[:,1]\n",
    "\n",
    "        # calculate metrics for this fold\n",
    "        y_pred = (oof_nb[val_idx] >= 0.5).astype(int)\n",
    "        fold_precision = precision_score(y_val, y_pred)\n",
    "        fold_recall = recall_score(y_val, y_pred)\n",
    "        fold_f1 = f1_score(y_val, y_pred)\n",
    "        fold_auc = roc_auc_score(y_val, oof_nb[val_idx])\n",
    "\n",
    "        print(f\"[NB] Fold {fold}: Precision={fold_precision:.5f}, Recall={fold_recall:.5f}, F1={fold_f1:.5f}, AUC={fold_auc:.5f}\")\n",
    "\n",
    "    # calculate overall metrics\n",
    "    y_pred_overall = (oof_nb >= 0.5).astype(int)\n",
    "    overall_precision = precision_score(y, y_pred_overall)\n",
    "    overall_recall = recall_score(y, y_pred_overall)\n",
    "    overall_f1 = f1_score(y, y_pred_overall)\n",
    "    overall_auc = roc_auc_score(y, oof_nb)\n",
    "    \n",
    "    return overall_precision, overall_recall, overall_f1, overall_auc, oof_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f551f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 224/224 [01:44<00:00,  2.14it/s]\n",
      "Batches: 100%|██████████| 224/224 [01:53<00:00,  1.97it/s]\n",
      "Batches: 100%|██████████| 56/56 [00:32<00:00,  1.74it/s]\n",
      "Batches: 100%|██████████| 56/56 [00:25<00:00,  2.21it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NB] Fold 1: Precision=0.37183, Recall=0.76301, F1=0.50000, AUC=0.84839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 224/224 [01:30<00:00,  2.48it/s]\n",
      "Batches: 100%|██████████| 224/224 [01:33<00:00,  2.39it/s]\n",
      "Batches: 100%|██████████| 56/56 [00:25<00:00,  2.22it/s]\n",
      "Batches: 100%|██████████| 56/56 [00:24<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NB] Fold 2: Precision=0.39414, Recall=0.69942, F1=0.50417, AUC=0.82468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 224/224 [01:42<00:00,  2.19it/s]\n",
      "Batches: 100%|██████████| 224/224 [01:35<00:00,  2.35it/s]\n",
      "Batches: 100%|██████████| 56/56 [00:26<00:00,  2.12it/s]\n",
      "Batches: 100%|██████████| 56/56 [00:28<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NB] Fold 3: Precision=0.45255, Recall=0.71676, F1=0.55481, AUC=0.83601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 224/224 [01:31<00:00,  2.46it/s]\n",
      "Batches: 100%|██████████| 224/224 [01:32<00:00,  2.43it/s]\n",
      "Batches: 100%|██████████| 56/56 [00:24<00:00,  2.28it/s]\n",
      "Batches: 100%|██████████| 56/56 [00:23<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NB] Fold 4: Precision=0.35562, Recall=0.67630, F1=0.46614, AUC=0.80667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 224/224 [01:32<00:00,  2.43it/s]\n",
      "Batches: 100%|██████████| 224/224 [01:31<00:00,  2.44it/s]\n",
      "Batches: 100%|██████████| 56/56 [00:25<00:00,  2.22it/s]\n",
      "Batches: 100%|██████████| 56/56 [00:23<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NB] Fold 5: Precision=0.40892, Recall=0.63218, F1=0.49661, AUC=0.79202\n",
      "\n",
      "============================================================\n",
      "Overall Naive Bayes Performance (5-Fold CV):\n",
      "============================================================\n",
      "Precision: 0.39374\n",
      "Recall:    0.69746\n",
      "F1 Score:  0.50333\n",
      "ROC AUC:   0.82152\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# test 5-fold CV on naive bayes\n",
    "precision, recall, f1, auc, oof_predictions = cv_nb_oof_metrics(X, y, n_splits=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Overall Naive Bayes Performance (5-Fold CV):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Precision: {precision:.5f}\")\n",
    "print(f\"Recall:    {recall:.5f}\")\n",
    "print(f\"F1 Score:  {f1:.5f}\")\n",
    "print(f\"ROC AUC:   {auc:.5f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d1f888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading CSV...\n",
      "[INFO] Data loaded: (17880, 18)\n",
      "[MAIN] Preparing X and y...\n",
      "[MAIN] Starting 5-fold CV with Random Forest...\n",
      "[CV] Starting RandomForest CV with 5 folds\n",
      "\n",
      "========== FOLD 1 ==========\n",
      "[CV] Building pipeline for this fold...\n",
      "[PIPE] Building full FeatureUnion...\n",
      "[CV] Fit-transform TRAIN data...\n",
      "[NUM] Running numeric features...\n",
      "[NUM] Output shape: (14304, 4)\n",
      "[CAT WRAP] Fitting...\n",
      "[CAT WRAP] Transforming...\n",
      "[CAT] Running categorical features...\n",
      "[CAT] Output shape: (14304, 5)\n",
      "[TXT] Running textual features...\n",
      "[SIM] Computing similarity for 14304 rows...\n",
      "[EMBED] Loading sentence transformer on mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 224/224 [03:43<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SIM] Computing similarity for 14304 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 224/224 [03:50<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TXT] Output shape: (14304, 16)\n",
      "[TFIDF] Combining text fields...\n",
      "[TFIDF] Combined text length: 14304\n",
      "[CV] TRAIN feature matrix shape: (14304, 5187)\n",
      "[CV] Transform VAL data...\n",
      "[NUM] Running numeric features...\n",
      "[NUM] Output shape: (3576, 4)\n",
      "[CAT WRAP] Transforming...\n",
      "[CAT] Running categorical features...\n",
      "[CAT] Output shape: (3576, 5)\n",
      "[TXT] Running textual features...\n",
      "[SIM] Computing similarity for 3576 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 56/56 [00:59<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SIM] Computing similarity for 3576 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 56/56 [01:00<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TXT] Output shape: (3576, 16)\n",
      "[TFIDF] Combining text fields...\n",
      "[TFIDF] Combined text length: 3576\n",
      "[CV] VAL feature matrix shape: (3576, 5187)\n",
      "[CV] Applying SVMSMOTE on train fold...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[CV] After SMOTE: X_train_res shape: (27222, 5187)\n",
      "[CV] Training RandomForestClassifier...\n",
      "[CV] Predicting on VAL fold...\n",
      "[FOLD 1] Precision=0.97521, Recall=0.68208, F1=0.80272, AUC=0.98941\n",
      "\n",
      "========== FOLD 2 ==========\n",
      "[CV] Building pipeline for this fold...\n",
      "[PIPE] Building full FeatureUnion...\n",
      "[CV] Fit-transform TRAIN data...\n",
      "[NUM] Running numeric features...\n",
      "[NUM] Output shape: (14304, 4)\n",
      "[CAT WRAP] Fitting...\n",
      "[CAT WRAP] Transforming...\n",
      "[CAT] Running categorical features...\n",
      "[CAT] Output shape: (14304, 5)\n",
      "[TXT] Running textual features...\n",
      "[SIM] Computing similarity for 14304 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 224/224 [04:17<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SIM] Computing similarity for 14304 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 224/224 [04:23<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TXT] Output shape: (14304, 16)\n",
      "[TFIDF] Combining text fields...\n",
      "[TFIDF] Combined text length: 14304\n",
      "[CV] TRAIN feature matrix shape: (14304, 5188)\n",
      "[CV] Transform VAL data...\n",
      "[NUM] Running numeric features...\n",
      "[NUM] Output shape: (3576, 4)\n",
      "[CAT WRAP] Transforming...\n",
      "[CAT] Running categorical features...\n",
      "[CAT] Output shape: (3576, 5)\n",
      "[TXT] Running textual features...\n",
      "[SIM] Computing similarity for 3576 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 56/56 [01:05<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SIM] Computing similarity for 3576 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 56/56 [01:09<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TXT] Output shape: (3576, 16)\n",
      "[TFIDF] Combining text fields...\n",
      "[TFIDF] Combined text length: 3576\n",
      "[CV] VAL feature matrix shape: (3576, 5188)\n",
      "[CV] Applying SVMSMOTE on train fold...\n",
      "[CV] After SMOTE: X_train_res shape: (27222, 5188)\n",
      "[CV] Training RandomForestClassifier...\n",
      "[CV] Predicting on VAL fold...\n",
      "[FOLD 2] Precision=0.96825, Recall=0.70520, F1=0.81605, AUC=0.98085\n",
      "\n",
      "========== FOLD 3 ==========\n",
      "[CV] Building pipeline for this fold...\n",
      "[PIPE] Building full FeatureUnion...\n",
      "[CV] Fit-transform TRAIN data...\n",
      "[NUM] Running numeric features...\n",
      "[NUM] Output shape: (14304, 4)\n",
      "[CAT WRAP] Fitting...\n",
      "[CAT WRAP] Transforming...\n",
      "[CAT] Running categorical features...\n",
      "[CAT] Output shape: (14304, 5)\n",
      "[TXT] Running textual features...\n",
      "[SIM] Computing similarity for 14304 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 224/224 [04:25<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SIM] Computing similarity for 14304 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 224/224 [04:36<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TXT] Output shape: (14304, 16)\n",
      "[TFIDF] Combining text fields...\n",
      "[TFIDF] Combined text length: 14304\n",
      "[CV] TRAIN feature matrix shape: (14304, 5189)\n",
      "[CV] Transform VAL data...\n",
      "[NUM] Running numeric features...\n",
      "[NUM] Output shape: (3576, 4)\n",
      "[CAT WRAP] Transforming...\n",
      "[CAT] Running categorical features...\n",
      "[CAT] Output shape: (3576, 5)\n",
      "[TXT] Running textual features...\n",
      "[SIM] Computing similarity for 3576 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 56/56 [01:15<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SIM] Computing similarity for 3576 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 56/56 [01:21<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TXT] Output shape: (3576, 16)\n",
      "[TFIDF] Combining text fields...\n",
      "[TFIDF] Combined text length: 3576\n",
      "[CV] VAL feature matrix shape: (3576, 5189)\n",
      "[CV] Applying SVMSMOTE on train fold...\n",
      "[CV] After SMOTE: X_train_res shape: (27222, 5189)\n",
      "[CV] Training RandomForestClassifier...\n",
      "[CV] Predicting on VAL fold...\n",
      "[FOLD 3] Precision=0.96581, Recall=0.65318, F1=0.77931, AUC=0.98736\n",
      "\n",
      "========== FOLD 4 ==========\n",
      "[CV] Building pipeline for this fold...\n",
      "[PIPE] Building full FeatureUnion...\n",
      "[CV] Fit-transform TRAIN data...\n",
      "[NUM] Running numeric features...\n",
      "[NUM] Output shape: (14304, 4)\n",
      "[CAT WRAP] Fitting...\n",
      "[CAT WRAP] Transforming...\n",
      "[CAT] Running categorical features...\n",
      "[CAT] Output shape: (14304, 5)\n",
      "[TXT] Running textual features...\n",
      "[SIM] Computing similarity for 14304 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 224/224 [04:07<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SIM] Computing similarity for 14304 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 224/224 [04:41<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TXT] Output shape: (14304, 16)\n",
      "[TFIDF] Combining text fields...\n",
      "[TFIDF] Combined text length: 14304\n",
      "[CV] TRAIN feature matrix shape: (14304, 5186)\n",
      "[CV] Transform VAL data...\n",
      "[NUM] Running numeric features...\n",
      "[NUM] Output shape: (3576, 4)\n",
      "[CAT WRAP] Transforming...\n",
      "[CAT] Running categorical features...\n",
      "[CAT] Output shape: (3576, 5)\n",
      "[TXT] Running textual features...\n",
      "[SIM] Computing similarity for 3576 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 56/56 [01:14<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SIM] Computing similarity for 3576 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 56/56 [06:47<00:00,  7.27s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TXT] Output shape: (3576, 16)\n",
      "[TFIDF] Combining text fields...\n",
      "[TFIDF] Combined text length: 3576\n",
      "[CV] VAL feature matrix shape: (3576, 5186)\n",
      "[CV] Applying SVMSMOTE on train fold...\n",
      "[CV] After SMOTE: X_train_res shape: (27222, 5186)\n",
      "[CV] Training RandomForestClassifier...\n",
      "[CV] Predicting on VAL fold...\n",
      "[FOLD 4] Precision=0.97619, Recall=0.71098, F1=0.82274, AUC=0.98923\n",
      "\n",
      "========== FOLD 5 ==========\n",
      "[CV] Building pipeline for this fold...\n",
      "[PIPE] Building full FeatureUnion...\n",
      "[CV] Fit-transform TRAIN data...\n",
      "[NUM] Running numeric features...\n",
      "[NUM] Output shape: (14304, 4)\n",
      "[CAT WRAP] Fitting...\n",
      "[CAT WRAP] Transforming...\n",
      "[CAT] Running categorical features...\n",
      "[CAT] Output shape: (14304, 5)\n",
      "[TXT] Running textual features...\n",
      "[SIM] Computing similarity for 14304 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 224/224 [03:32<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SIM] Computing similarity for 14304 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 224/224 [04:33<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TXT] Output shape: (14304, 16)\n",
      "[TFIDF] Combining text fields...\n",
      "[TFIDF] Combined text length: 14304\n",
      "[CV] TRAIN feature matrix shape: (14304, 5188)\n",
      "[CV] Transform VAL data...\n",
      "[NUM] Running numeric features...\n",
      "[NUM] Output shape: (3576, 4)\n",
      "[CAT WRAP] Transforming...\n",
      "[CAT] Running categorical features...\n",
      "[CAT] Output shape: (3576, 5)\n",
      "[TXT] Running textual features...\n",
      "[SIM] Computing similarity for 3576 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 56/56 [01:17<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SIM] Computing similarity for 3576 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 56/56 [01:14<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TXT] Output shape: (3576, 16)\n",
      "[TFIDF] Combining text fields...\n",
      "[TFIDF] Combined text length: 3576\n",
      "[CV] VAL feature matrix shape: (3576, 5188)\n",
      "[CV] Applying SVMSMOTE on train fold...\n",
      "[CV] After SMOTE: X_train_res shape: (27224, 5188)\n",
      "[CV] Training RandomForestClassifier...\n",
      "[CV] Predicting on VAL fold...\n",
      "[FOLD 5] Precision=0.97368, Recall=0.63793, F1=0.77083, AUC=0.98328\n",
      "\n",
      "==== OVERALL OUT-OF-FOLD METRICS ====\n",
      "Precision: 0.97183\n",
      "Recall:    0.67788\n",
      "F1 Score:  0.79833\n",
      "ROC AUC:   0.98603\n",
      "=====================================\n",
      "\n",
      "============================================================\n",
      "Overall Random Forest Performance (5-Fold CV):\n",
      "============================================================\n",
      "Precision: 0.97183\n",
      "Recall:    0.67788\n",
      "F1 Score:  0.79833\n",
      "ROC AUC:   0.98603\n",
      "============================================================\n",
      "[MAIN] Finished.\n"
     ]
    }
   ],
   "source": [
    "def cv_rf_oof_metrics(X, y, n_splits=5):\n",
    "    print(\"[CV] Starting RandomForest CV with\", n_splits, \"folds\")\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    oof = np.zeros(len(y), dtype=float)\n",
    "    metrics = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), start=1):\n",
    "        print(f\"\\n========== FOLD {fold} ==========\")\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        print(\"[CV] Building pipeline for this fold...\")\n",
    "        features = build_complete_pipeline()\n",
    "\n",
    "        print(\"[CV] Fit-transform TRAIN data...\")\n",
    "        X_train_f = features.fit_transform(X_train, y_train)\n",
    "        print(\"[CV] TRAIN feature matrix shape:\", X_train_f.shape)\n",
    "\n",
    "        print(\"[CV] Transform VAL data...\")\n",
    "        X_val_f = features.transform(X_val)\n",
    "        print(\"[CV] VAL feature matrix shape:\", X_val_f.shape)\n",
    "\n",
    "        # SMOTE on training fold\n",
    "        print(\"[CV] Applying SVMSMOTE on train fold...\")\n",
    "        smote = SVMSMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train_f, y_train)\n",
    "        print(\"[CV] After SMOTE: X_train_res shape:\", X_train_res.shape)\n",
    "\n",
    "        # Random Forest\n",
    "        print(\"[CV] Training RandomForestClassifier...\")\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=None,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train_res, y_train_res)\n",
    "\n",
    "        # Predictions\n",
    "        print(\"[CV] Predicting on VAL fold...\")\n",
    "        val_proba = model.predict_proba(X_val_f)[:, 1]\n",
    "        oof[val_idx] = val_proba\n",
    "        val_pred = (val_proba >= 0.5).astype(int)\n",
    "\n",
    "        # Metrics\n",
    "        fold_precision = precision_score(y_val, val_pred)\n",
    "        fold_recall = recall_score(y_val, val_pred)\n",
    "        fold_f1 = f1_score(y_val, val_pred)\n",
    "        fold_auc = roc_auc_score(y_val, val_proba)\n",
    "\n",
    "        print(f\"[FOLD {fold}] Precision={fold_precision:.5f}, \"\n",
    "              f\"Recall={fold_recall:.5f}, F1={fold_f1:.5f}, AUC={fold_auc:.5f}\")\n",
    "\n",
    "        metrics.append((fold_precision, fold_recall, fold_f1, fold_auc))\n",
    "\n",
    "    # Aggregate metrics\n",
    "    metrics = np.array(metrics)\n",
    "    overall_precision = metrics[:, 0].mean()\n",
    "    overall_recall = metrics[:, 1].mean()\n",
    "    overall_f1 = metrics[:, 2].mean()\n",
    "    overall_auc = metrics[:, 3].mean()\n",
    "\n",
    "    print(\"\\n==== OVERALL OUT-OF-FOLD METRICS ====\")\n",
    "    print(f\"Precision: {overall_precision:.5f}\")\n",
    "    print(f\"Recall:    {overall_recall:.5f}\")\n",
    "    print(f\"F1 Score:  {overall_f1:.5f}\")\n",
    "    print(f\"ROC AUC:   {overall_auc:.5f}\")\n",
    "    print(\"=====================================\")\n",
    "\n",
    "    return overall_precision, overall_recall, overall_f1, overall_auc, oof\n",
    "\n",
    "print(\"[MAIN] Starting 5-fold CV with Random Forest...\")\n",
    "precision, recall, f1, auc, oof_predictions = cv_rf_oof_metrics(X, y, n_splits=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Overall Random Forest Performance (5-Fold CV):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Precision: {precision:.5f}\")\n",
    "print(f\"Recall:    {recall:.5f}\")\n",
    "print(f\"F1 Score:  {f1:.5f}\")\n",
    "print(f\"ROC AUC:   {auc:.5f}\")\n",
    "print(\"=\"*60)\n",
    "print(\"[MAIN] Finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02116f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_xgb_oof_metrics(X, y, n_splits=5):\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    oof_xgb = np.zeros(len(y), dtype=float)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # build features per fold\n",
    "        features = build_complete_pipeline()\n",
    "        X_train_f = features.fit_transform(X_train)\n",
    "        X_val_f = features.transform(X_val)\n",
    "\n",
    "        # oversample minority class (fraud) on the train data\n",
    "        smote = SVMSMOTE(random_state=42)\n",
    "        X_res, y_res = smote.fit_resample(X_train_f, y_train)\n",
    "\n",
    "        # define xgboost model (binary classifier)\n",
    "        xgb = XGBClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            tree_method=\"hist\",\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # fit on resampled training data\n",
    "        xgb.fit(X_res, y_res)\n",
    "\n",
    "        # out-of-fold predicted probabilities for class 1 (fraud)\n",
    "        oof_xgb[val_idx] = xgb.predict_proba(X_val_f)[:, 1]\n",
    "\n",
    "        # hard predictions for per-fold metrics\n",
    "        y_pred = (oof_xgb[val_idx] >= 0.5).astype(int)\n",
    "\n",
    "        fold_precision = precision_score(y_val, y_pred, zero_division=0)\n",
    "        fold_recall = recall_score(y_val, y_pred, zero_division=0)\n",
    "        fold_f1 = f1_score(y_val, y_pred, zero_division=0)\n",
    "        fold_auc = roc_auc_score(y_val, oof_xgb[val_idx])\n",
    "\n",
    "        print(\n",
    "            f\"[XGB] Fold {fold}: \"\n",
    "            f\"Precision={fold_precision:.5f}, \"\n",
    "            f\"Recall={fold_recall:.5f}, \"\n",
    "            f\"F1={fold_f1:.5f}, \"\n",
    "            f\"AUC={fold_auc:.5f}\"\n",
    "        )\n",
    "\n",
    "    # overall metrics across all folds (using oof predictions)\n",
    "    y_pred_overall = (oof_xgb >= 0.5).astype(int)\n",
    "    overall_precision = precision_score(y, y_pred_overall, zero_division=0)\n",
    "    overall_recall = recall_score(y, y_pred_overall, zero_division=0)\n",
    "    overall_f1 = f1_score(y, y_pred_overall, zero_division=0)\n",
    "    overall_auc = roc_auc_score(y, oof_xgb)\n",
    "\n",
    "    return overall_precision, overall_recall, overall_f1, overall_auc, oof_xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64c97ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 5-fold CV on XGBoost \n",
    "precision, recall, f1, auc, oof_predictions = cv_xgb_oof_metrics(X, y, n_splits=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Overall XGBoost Performance (5-Fold CV):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Precision: {precision:.5f}\")\n",
    "print(f\"Recall:    {recall:.5f}\")\n",
    "print(f\"F1 Score:  {f1:.5f}\")\n",
    "print(f\"ROC AUC:   {auc:.5f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60864d4a",
   "metadata": {},
   "source": [
    "#### Deep Learning Models\n",
    "\n",
    "Due to compute reasons, we ran the deep learning models with 3 folds instead. They also won't use minority oversampling techniques, relying on class weights instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110a6975",
   "metadata": {},
   "source": [
    "_Bi-directional LSTM_\n",
    "- incorporates engineered features + raw text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d3acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hybrid_model(engineered_feature_dim, vocab_size=10000, max_length=200):\n",
    "    # handle our feature engineering columns\n",
    "    engineered_input = Input(shape=(engineered_feature_dim,), name='engineered_features')\n",
    "    \n",
    "    x_eng = Dense(512, activation='relu')(engineered_input)\n",
    "    x_eng = Dropout(0.4)(x_eng)\n",
    "    x_eng = Dense(256, activation='relu')(x_eng)\n",
    "    x_eng = Dropout(0.3)(x_eng)\n",
    "    \n",
    "    # handle the text inputs for lstm to learn in addition to engineered features\n",
    "    # shared embedding dimension\n",
    "    embedding_dim = 128\n",
    "    lstm_units = 128\n",
    "    \n",
    "    # text inputs\n",
    "    title_input = Input(shape=(1,), dtype=tf.string, name='title')\n",
    "    company_profile_input = Input(shape=(1,), dtype=tf.string, name='company_profile')\n",
    "    description_input = Input(shape=(1,), dtype=tf.string, name='description')\n",
    "    requirements_input = Input(shape=(1,), dtype=tf.string, name='requirements')\n",
    "    benefits_input = Input(shape=(1,), dtype=tf.string, name='benefits')\n",
    "    \n",
    "    # shared text vectorization layer\n",
    "    text_vectorizer = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=max_length\n",
    "    )\n",
    "    \n",
    "    # shared embedding layer\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        mask_zero=True\n",
    "    )\n",
    "    \n",
    "    # process each text column through bi-lstm\n",
    "    def process_text(text_input):\n",
    "        x = text_vectorizer(text_input)\n",
    "        x = embedding_layer(x)\n",
    "        x = Bidirectional(LSTM(lstm_units, dropout=0.3, recurrent_dropout=0.2))(x)\n",
    "        return x\n",
    "    \n",
    "    title_features = process_text(title_input)\n",
    "    company_profile_features = process_text(company_profile_input)\n",
    "    description_features = process_text(description_input)\n",
    "    requirements_features = process_text(requirements_input)\n",
    "    benefits_features = process_text(benefits_input)\n",
    "    \n",
    "    # concatenate all text features\n",
    "    x_text = Concatenate()([\n",
    "        title_features,\n",
    "        company_profile_features,\n",
    "        description_features,\n",
    "        requirements_features,\n",
    "        benefits_features\n",
    "    ])\n",
    "    \n",
    "    # combine engineered features and text features\n",
    "    combined = Concatenate()([x_eng, x_text])\n",
    "    \n",
    "    x = Dense(256, activation='relu')(combined)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "    \n",
    "    # build the model\n",
    "    model = Model(\n",
    "        inputs=[\n",
    "            engineered_input,\n",
    "            title_input,\n",
    "            company_profile_input,\n",
    "            description_input,\n",
    "            requirements_input,\n",
    "            benefits_input\n",
    "        ],\n",
    "        outputs=output\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc'), # include accuracy just for sanity check\n",
    "                 tf.keras.metrics.Precision(name='precision'),\n",
    "                 tf.keras.metrics.Recall(name='recall')]\n",
    "    )\n",
    "    \n",
    "    return model, text_vectorizer\n",
    "\n",
    "\n",
    "def cv_hybrid_with_class_weights(X, y, n_splits=5):\n",
    "    \"\"\"\n",
    "    stratified 5-fold cv with class weights (no smote)\n",
    "    \"\"\"\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    oof_predictions = np.zeros(len(y), dtype=float)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Fold {fold}/{n_splits}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # split data\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # build features per fold\n",
    "        print(\"Building engineered features...\")\n",
    "        feature_pipeline = build_complete_pipeline()\n",
    "        X_train_engineered = feature_pipeline.fit_transform(X_train)\n",
    "        X_val_engineered = feature_pipeline.transform(X_val)\n",
    "        \n",
    "        # convert to dense\n",
    "        if hasattr(X_train_engineered, 'toarray'):\n",
    "            X_train_engineered = X_train_engineered.toarray()\n",
    "            X_val_engineered = X_val_engineered.toarray()\n",
    "        \n",
    "        # compute class weights\n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.unique(y_train),\n",
    "            y=y_train\n",
    "        )\n",
    "        class_weight_dict = {i: w for i, w in enumerate(class_weights)}\n",
    "        print(f\"Class weights: {class_weight_dict}\")\n",
    "        \n",
    "        # prepare text data\n",
    "        print(\"Preparing text data...\")\n",
    "        # convert to numpy arrays of strings (required for keras with mixed input types)\n",
    "        X_train_text = {\n",
    "            'title': np.array(X_train['title'].fillna('').astype(str).tolist(), dtype=object),\n",
    "            'company_profile': np.array(X_train['company_profile'].fillna('').astype(str).tolist(), dtype=object),\n",
    "            'description': np.array(X_train['description'].fillna('').astype(str).tolist(), dtype=object),\n",
    "            'requirements': np.array(X_train['requirements'].fillna('').astype(str).tolist(), dtype=object),\n",
    "            'benefits': np.array(X_train['benefits'].fillna('').astype(str).tolist(), dtype=object)\n",
    "        }\n",
    "\n",
    "        X_val_text = {\n",
    "            'title': np.array(X_val['title'].fillna('').astype(str).tolist(), dtype=object),\n",
    "            'company_profile': np.array(X_val['company_profile'].fillna('').astype(str).tolist(), dtype=object),\n",
    "            'description': np.array(X_val['description'].fillna('').astype(str).tolist(), dtype=object),\n",
    "            'requirements': np.array(X_val['requirements'].fillna('').astype(str).tolist(), dtype=object),\n",
    "            'benefits': np.array(X_val['benefits'].fillna('').astype(str).tolist(), dtype=object)\n",
    "        }\n",
    "        \n",
    "        # build model\n",
    "        print(\"Building hybrid model...\")\n",
    "        model, text_vectorizer = build_hybrid_model(\n",
    "            engineered_feature_dim=X_train_engineered.shape[1]\n",
    "        )\n",
    "        \n",
    "        # adapt text vectorizer to training data\n",
    "        all_text = np.concatenate([\n",
    "            X_train['title'].fillna('').values,\n",
    "            X_train['company_profile'].fillna('').values,\n",
    "            X_train['description'].fillna('').values,\n",
    "            X_train['requirements'].fillna('').values,\n",
    "            X_train['benefits'].fillna('').values\n",
    "        ])\n",
    "        text_vectorizer.adapt(all_text)\n",
    "        \n",
    "        # prepare inputs\n",
    "        train_inputs = {\n",
    "            'engineered_features': X_train_engineered,\n",
    "            **X_train_text\n",
    "        }\n",
    "        \n",
    "        val_inputs = {\n",
    "            'engineered_features': X_val_engineered,\n",
    "            **X_val_text\n",
    "        }\n",
    "        \n",
    "        # train the model\n",
    "        print(\"Training model...\")\n",
    "        history = model.fit(\n",
    "            train_inputs,\n",
    "            y_train,\n",
    "            validation_data=(val_inputs, y_val),\n",
    "            class_weight=class_weight_dict,  # handles imbalance\n",
    "            epochs=3, # use 3 epochs for validation since 20 takes too long\n",
    "            batch_size=32,\n",
    "            verbose=1,\n",
    "            callbacks=[\n",
    "                EarlyStopping(monitor='val_auc', patience=2, mode='max', restore_best_weights=True),  # patience=2 for 3 epochs\n",
    "                ReduceLROnPlateau(monitor='val_loss', patience=1, factor=0.5)  # patience=1 for 3 epochs\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # predict\n",
    "        oof_predictions[val_idx] = model.predict(val_inputs, verbose=0).flatten()\n",
    "        \n",
    "        # metrics\n",
    "        y_pred = (oof_predictions[val_idx] >= 0.5).astype(int)\n",
    "        fold_precision = precision_score(y_val, y_pred)\n",
    "        fold_recall = recall_score(y_val, y_pred)\n",
    "        fold_f1 = f1_score(y_val, y_pred)\n",
    "        fold_auc = roc_auc_score(y_val, oof_predictions[val_idx])\n",
    "        \n",
    "        print(f\"\\n[Hybrid] Fold {fold}: Precision={fold_precision:.5f}, Recall={fold_recall:.5f}, F1={fold_f1:.5f}, AUC={fold_auc:.5f}\")\n",
    "        \n",
    "        # clear memory\n",
    "        del model\n",
    "        tf.keras.backend.clear_session()\n",
    "    \n",
    "    # overall metrics\n",
    "    y_pred_overall = (oof_predictions >= 0.5).astype(int)\n",
    "    overall_precision = precision_score(y, y_pred_overall)\n",
    "    overall_recall = recall_score(y, y_pred_overall)\n",
    "    overall_f1 = f1_score(y, y_pred_overall)\n",
    "    overall_auc = roc_auc_score(y, oof_predictions)\n",
    "    \n",
    "    return overall_precision, overall_recall, overall_f1, overall_auc, oof_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e30f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Fold 1/3\n",
      "============================================================\n",
      "Building engineered features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d28699a27942deaa2cee88d3d7d1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81be083f165d4704a16718c173d84714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204ecb0ffd5a452a8bcdb216f49b47f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f934c73d18f4201a8854d37a63c482c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240b21e1349a461e81834eea3dd2279f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc2a7731d5142d5bde3b29f20e85ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d58b624b80543f5af0af02d0c4be6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3ff737d18e42f3bdfb52ada5b25c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f491d0414c4cf68986ed14f6b6ead2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c018535a0482483fab9ef81288c6f277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2434069c47624b4cb4dc70e5a9137728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using cuda for sentence transformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003482ab61a0482988944218f087c777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b780bef7a49c46c886b92b98ea28e1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8412876ac4bd4c65910ea476fde8f45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf6d4d04e5a4cbab3752e8c9e4fb745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.5254805149003703, 1: 10.311418685121108}\n",
      "Preparing text data...\n",
      "Building hybrid model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1763270710.209881      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12944 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1763270710.210487      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/3\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2017s\u001b[0m 5s/step - accuracy: 0.8783 - auc: 0.8851 - loss: 0.4120 - precision: 0.2559 - recall: 0.7127 - val_accuracy: 0.9151 - val_auc: 0.9910 - val_loss: 0.1633 - val_precision: 0.3610 - val_recall: 0.9826 - learning_rate: 0.0010\n",
      "Epoch 2/3\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1966s\u001b[0m 5s/step - accuracy: 0.9586 - auc: 0.9918 - loss: 0.0904 - precision: 0.5417 - recall: 0.9731 - val_accuracy: 0.9757 - val_auc: 0.9810 - val_loss: 0.0759 - val_precision: 0.6857 - val_recall: 0.9167 - learning_rate: 0.0010\n",
      "Epoch 3/3\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1956s\u001b[0m 5s/step - accuracy: 0.9906 - auc: 0.9988 - loss: 0.0246 - precision: 0.8425 - recall: 0.9961 - val_accuracy: 0.9836 - val_auc: 0.9767 - val_loss: 0.0642 - val_precision: 0.7969 - val_recall: 0.8854 - learning_rate: 0.0010\n",
      "\n",
      "[Hybrid] Fold 1: Precision=0.36097, Recall=0.98264, F1=0.52799, AUC=0.99210\n",
      "\n",
      "============================================================\n",
      "Fold 2/3\n",
      "============================================================\n",
      "Building engineered features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a320c86fe87444848b54ddf5d46c90d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ee907d691449a0aac370f4b6e8b907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e700a70cf04e5bb9d5cfe1bb446d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41f611bd4574b66a30ce280a1982375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.5254341884862911, 1: 10.329289428076256}\n",
      "Preparing text data...\n",
      "Building hybrid model...\n",
      "Training model...\n",
      "Epoch 1/3\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1999s\u001b[0m 5s/step - accuracy: 0.9008 - auc: 0.8598 - loss: 0.4352 - precision: 0.2654 - recall: 0.6615 - val_accuracy: 0.9413 - val_auc: 0.9915 - val_loss: 0.1243 - val_precision: 0.4512 - val_recall: 0.9758 - learning_rate: 0.0010\n",
      "Epoch 2/3\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1963s\u001b[0m 5s/step - accuracy: 0.9685 - auc: 0.9940 - loss: 0.0925 - precision: 0.6145 - recall: 0.9664 - val_accuracy: 0.9628 - val_auc: 0.9838 - val_loss: 0.0877 - val_precision: 0.5714 - val_recall: 0.9273 - learning_rate: 0.0010\n",
      "Epoch 3/3\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1955s\u001b[0m 5s/step - accuracy: 0.9785 - auc: 0.9917 - loss: 0.0747 - precision: 0.6941 - recall: 0.9727 - val_accuracy: 0.9404 - val_auc: 0.9861 - val_loss: 0.1826 - val_precision: 0.4476 - val_recall: 0.9758 - learning_rate: 0.0010\n",
      "\n",
      "[Hybrid] Fold 2: Precision=0.45120, Recall=0.97578, F1=0.61707, AUC=0.99232\n",
      "\n",
      "============================================================\n",
      "Fold 3/3\n",
      "============================================================\n",
      "Building engineered features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc7f4fd16fe418bae09dcc473d20b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11349ca60b1044be8404dee877f03ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af2a802a1374419af4467fa7e16be7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d81f7770820342638287d5c37f2254c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.5254341884862911, 1: 10.329289428076256}\n",
      "Preparing text data...\n",
      "Building hybrid model...\n",
      "Training model...\n",
      "Epoch 1/3\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1980s\u001b[0m 5s/step - accuracy: 0.8447 - auc: 0.8918 - loss: 0.4462 - precision: 0.2474 - recall: 0.7996 - val_accuracy: 0.9332 - val_auc: 0.9922 - val_loss: 0.1574 - val_precision: 0.4183 - val_recall: 0.9654 - learning_rate: 0.0010\n",
      "Epoch 2/3\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1948s\u001b[0m 5s/step - accuracy: 0.9695 - auc: 0.9959 - loss: 0.0788 - precision: 0.6227 - recall: 0.9697 - val_accuracy: 0.9661 - val_auc: 0.9916 - val_loss: 0.0697 - val_precision: 0.5969 - val_recall: 0.9273 - learning_rate: 0.0010\n",
      "Epoch 3/3\n",
      "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1945s\u001b[0m 5s/step - accuracy: 0.9856 - auc: 0.9978 - loss: 0.0437 - precision: 0.7798 - recall: 0.9934 - val_accuracy: 0.9824 - val_auc: 0.9840 - val_loss: 0.0634 - val_precision: 0.7674 - val_recall: 0.9135 - learning_rate: 0.0010\n",
      "\n",
      "[Hybrid] Fold 3: Precision=0.41829, Recall=0.96540, F1=0.58368, AUC=0.99230\n",
      "\n",
      "============================================================\n",
      "Overall Bidirect-LSTM Performance (3-Fold CV):\n",
      "============================================================\n",
      "Precision: 0.40655\n",
      "Recall:    0.97460\n",
      "F1 Score:  0.57376\n",
      "ROC AUC:   0.99184\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# test 3-fold CV on LSTM\n",
    "precision, recall, f1, auc, oof_predictions = cv_hybrid_with_class_weights(X, y, n_splits=3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Overall Bidirect-LSTM Performance (3-Fold CV):\") # 3 fold for validation since 5 fold is too slow\n",
    "print(\"=\"*60)\n",
    "print(f\"Precision: {precision:.5f}\")\n",
    "print(f\"Recall:    {recall:.5f}\")\n",
    "print(f\"F1 Score:  {f1:.5f}\")\n",
    "print(f\"ROC AUC:   {auc:.5f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3612044",
   "metadata": {},
   "source": [
    "_DistilBERT_\n",
    "- done solely on text columns to see how it fares against our customised BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f761e5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading CSV...\n",
      "[INFO] Data loaded: (17880, 18)\n",
      "[INFO] Building combined text column...\n",
      "[INFO] Example text sample:\n",
      " Marketing Intern [SEP] US, NY, New York [SEP] Marketing [SEP] Other [SEP] Internship [SEP] nan [SEP] nan [SEP] Marketing [SEP] We're Food52, and we've created a groundbreaking and award-winning cooking site. We support, connect, and celebrate home cooks, and give them everything they need in one place.We have a top editorial, business, and engineering team. We're focused on using technology to find new and better ways to connect people around their specific food interests, and to offer them supe ...\n",
      "\n",
      "[INFO] Using device: mps\n",
      "[INFO] Loading DistilBERT tokenizer...\n",
      "\n",
      "============================================================\n",
      "[FOLD 1/3]\n",
      "============================================================\n",
      "[FOLD 1] Train size: 11920 | Val size: 5960\n",
      "[FOLD 1] Loading DistilBERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TRAIN] Epoch 1/3\n",
      "  Step    1 | Batch loss: 0.6608\n",
      "  Step   50 | Batch loss: 0.0417\n",
      "  Step  100 | Batch loss: 0.0609\n",
      "  Step  150 | Batch loss: 0.2594\n",
      "  Step  200 | Batch loss: 0.2317\n",
      "  Step  250 | Batch loss: 0.0163\n",
      "  Step  300 | Batch loss: 0.0138\n",
      "  Step  350 | Batch loss: 0.0062\n",
      "  Step  400 | Batch loss: 0.0325\n",
      "  Step  450 | Batch loss: 0.0130\n",
      "  Step  500 | Batch loss: 0.0492\n",
      "  Step  550 | Batch loss: 0.1361\n",
      "  Step  600 | Batch loss: 0.0094\n",
      "  Step  650 | Batch loss: 0.0274\n",
      "  Step  700 | Batch loss: 0.0097\n",
      "[TRAIN] Epoch 1 finished. Avg loss: 0.1183\n",
      "\n",
      "[EVAL] Running on validation set...\n",
      "[EVAL] Precision: 0.74917\n",
      "[EVAL] Recall:    0.78819\n",
      "[EVAL] F1:        0.76819\n",
      "[EVAL] ROC AUC:   0.98164\n",
      "[FOLD 1] New best F1: 0.76819 (prev 0.00000)\n",
      "\n",
      "[TRAIN] Epoch 2/3\n",
      "  Step    1 | Batch loss: 0.3793\n",
      "  Step   50 | Batch loss: 0.0038\n",
      "  Step  100 | Batch loss: 0.0376\n",
      "  Step  150 | Batch loss: 0.2105\n",
      "  Step  200 | Batch loss: 0.0141\n",
      "  Step  250 | Batch loss: 0.0287\n",
      "  Step  300 | Batch loss: 0.1520\n",
      "  Step  350 | Batch loss: 0.0075\n",
      "  Step  400 | Batch loss: 0.0244\n",
      "  Step  450 | Batch loss: 0.0261\n",
      "  Step  500 | Batch loss: 0.0108\n",
      "  Step  550 | Batch loss: 0.0073\n",
      "  Step  600 | Batch loss: 0.0734\n",
      "  Step  650 | Batch loss: 0.0022\n",
      "  Step  700 | Batch loss: 0.0171\n",
      "[TRAIN] Epoch 2 finished. Avg loss: 0.0505\n",
      "\n",
      "[EVAL] Running on validation set...\n",
      "[EVAL] Precision: 0.95197\n",
      "[EVAL] Recall:    0.75694\n",
      "[EVAL] F1:        0.84333\n",
      "[EVAL] ROC AUC:   0.98806\n",
      "[FOLD 1] New best F1: 0.84333 (prev 0.76819)\n",
      "\n",
      "[TRAIN] Epoch 3/3\n",
      "  Step    1 | Batch loss: 0.0076\n",
      "  Step   50 | Batch loss: 0.0091\n",
      "  Step  100 | Batch loss: 0.0026\n",
      "  Step  150 | Batch loss: 0.0121\n",
      "  Step  200 | Batch loss: 0.0026\n",
      "  Step  250 | Batch loss: 0.0043\n",
      "  Step  300 | Batch loss: 0.0010\n",
      "  Step  350 | Batch loss: 0.0217\n",
      "  Step  400 | Batch loss: 0.0016\n",
      "  Step  450 | Batch loss: 0.0078\n",
      "  Step  500 | Batch loss: 0.0014\n",
      "  Step  550 | Batch loss: 0.0281\n",
      "  Step  600 | Batch loss: 0.0145\n",
      "  Step  650 | Batch loss: 0.0027\n",
      "  Step  700 | Batch loss: 0.0014\n",
      "[TRAIN] Epoch 3 finished. Avg loss: 0.0268\n",
      "\n",
      "[EVAL] Running on validation set...\n",
      "[EVAL] Precision: 0.88172\n",
      "[EVAL] Recall:    0.85417\n",
      "[EVAL] F1:        0.86772\n",
      "[EVAL] ROC AUC:   0.98922\n",
      "[FOLD 1] New best F1: 0.86772 (prev 0.84333)\n",
      "\n",
      "[FOLD 1] Best metrics:\n",
      "  Precision: 0.88172\n",
      "  Recall:    0.85417\n",
      "  F1:        0.86772\n",
      "  ROC AUC:   0.98922\n",
      "\n",
      "============================================================\n",
      "[FOLD 2/3]\n",
      "============================================================\n",
      "[FOLD 2] Train size: 11920 | Val size: 5960\n",
      "[FOLD 2] Loading DistilBERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TRAIN] Epoch 1/3\n",
      "  Step    1 | Batch loss: 0.7212\n",
      "  Step   50 | Batch loss: 0.3663\n",
      "  Step  100 | Batch loss: 0.0228\n",
      "  Step  150 | Batch loss: 0.3797\n",
      "  Step  200 | Batch loss: 0.1537\n",
      "  Step  250 | Batch loss: 0.0425\n",
      "  Step  300 | Batch loss: 0.0364\n",
      "  Step  350 | Batch loss: 0.3200\n",
      "  Step  400 | Batch loss: 0.0084\n",
      "  Step  450 | Batch loss: 0.0220\n",
      "  Step  500 | Batch loss: 0.5117\n",
      "  Step  550 | Batch loss: 0.0079\n",
      "  Step  600 | Batch loss: 0.0135\n",
      "  Step  650 | Batch loss: 0.1147\n",
      "  Step  700 | Batch loss: 0.0340\n",
      "[TRAIN] Epoch 1 finished. Avg loss: 0.1194\n",
      "\n",
      "[EVAL] Running on validation set...\n",
      "[EVAL] Precision: 0.95775\n",
      "[EVAL] Recall:    0.47059\n",
      "[EVAL] F1:        0.63109\n",
      "[EVAL] ROC AUC:   0.96852\n",
      "[FOLD 2] New best F1: 0.63109 (prev 0.00000)\n",
      "\n",
      "[TRAIN] Epoch 2/3\n",
      "  Step    1 | Batch loss: 0.2329\n",
      "  Step   50 | Batch loss: 0.0126\n",
      "  Step  100 | Batch loss: 0.0280\n",
      "  Step  150 | Batch loss: 0.0123\n",
      "  Step  200 | Batch loss: 0.0282\n",
      "  Step  250 | Batch loss: 0.0309\n",
      "  Step  300 | Batch loss: 0.0995\n",
      "  Step  350 | Batch loss: 0.0098\n",
      "  Step  400 | Batch loss: 0.0017\n",
      "  Step  450 | Batch loss: 0.0057\n",
      "  Step  500 | Batch loss: 0.2650\n",
      "  Step  550 | Batch loss: 0.0094\n",
      "  Step  600 | Batch loss: 0.0105\n",
      "  Step  650 | Batch loss: 0.0211\n",
      "  Step  700 | Batch loss: 0.0030\n",
      "[TRAIN] Epoch 2 finished. Avg loss: 0.0500\n",
      "\n",
      "[EVAL] Running on validation set...\n",
      "[EVAL] Precision: 0.78457\n",
      "[EVAL] Recall:    0.84429\n",
      "[EVAL] F1:        0.81333\n",
      "[EVAL] ROC AUC:   0.98304\n",
      "[FOLD 2] New best F1: 0.81333 (prev 0.63109)\n",
      "\n",
      "[TRAIN] Epoch 3/3\n",
      "  Step    1 | Batch loss: 0.0316\n",
      "  Step   50 | Batch loss: 0.0028\n",
      "  Step  100 | Batch loss: 0.0092\n",
      "  Step  150 | Batch loss: 0.0344\n",
      "  Step  200 | Batch loss: 0.0254\n",
      "  Step  250 | Batch loss: 0.1628\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 226\u001b[39m\n",
      "\u001b[32m    224\u001b[39m \u001b[38;5;66;03m# Train for EPOCHS on this fold\u001b[39;00m\n",
      "\u001b[32m    225\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    227\u001b[39m     precision, recall, f1, auc = eval_on_loader(model, val_loader, device)\n",
      "\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m f1 > best_f1:\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 134\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, train_loader, optimizer, device, epoch, num_epochs)\u001b[39m\n",
      "\u001b[32m    131\u001b[39m loss = outputs.loss\n",
      "\u001b[32m    133\u001b[39m loss.backward()\n",
      "\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    135\u001b[39m optimizer.zero_grad()\n",
      "\u001b[32m    137\u001b[39m total_loss += loss.item()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/FakeJobPostings/fakejobvenv/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n",
      "\u001b[32m    380\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m    381\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n",
      "\u001b[32m    382\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m    383\u001b[39m             )\n",
      "\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    386\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n",
      "\u001b[32m    388\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/FakeJobPostings/fakejobvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n",
      "\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n",
      "\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n",
      "\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n",
      "\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/FakeJobPostings/fakejobvenv/lib/python3.11/site-packages/transformers/optimization.py:466\u001b[39m, in \u001b[36mAdamW.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n",
      "\u001b[32m    462\u001b[39m state[\u001b[33m\"\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m\"\u001b[39m] += \u001b[32m1\u001b[39m\n",
      "\u001b[32m    464\u001b[39m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n",
      "\u001b[32m    465\u001b[39m \u001b[38;5;66;03m# In-place operations to update the averages at the same time\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m \u001b[43mexp_avg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m.add_(grad, alpha=(\u001b[32m1.0\u001b[39m - beta1))\n",
      "\u001b[32m    467\u001b[39m exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=\u001b[32m1.0\u001b[39m - beta2)\n",
      "\u001b[32m    468\u001b[39m denom = exp_avg_sq.sqrt().add_(group[\u001b[33m\"\u001b[39m\u001b[33meps\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# config\n",
    "CSV_PATH   = \"../data/fake_job_postings.csv\"\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "N_SPLITS   = 3\n",
    "EPOCHS     = 3\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 256\n",
    "LR        = 2e-5\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# load data\n",
    "print(\"[INFO] Loading CSV...\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"[INFO] Data loaded:\", df.shape)\n",
    "\n",
    "# target\n",
    "labels = df[\"fraudulent\"].astype(int).values\n",
    "\n",
    "# build text input (distilbert will see only this)\n",
    "def make_text(row):\n",
    "    parts = [\n",
    "        str(row.get(\"title\", \"\")),\n",
    "        str(row.get(\"location\", \"\")),\n",
    "        str(row.get(\"department\", \"\")),\n",
    "        str(row.get(\"employment_type\", \"\")),\n",
    "        str(row.get(\"required_experience\", \"\")),\n",
    "        str(row.get(\"required_education\", \"\")),\n",
    "        str(row.get(\"industry\", \"\")),\n",
    "        str(row.get(\"function\", \"\")),\n",
    "        str(row.get(\"company_profile\", \"\")),\n",
    "        str(row.get(\"description\", \"\")),\n",
    "        str(row.get(\"requirements\", \"\")),\n",
    "        str(row.get(\"benefits\", \"\")),\n",
    "    ]\n",
    "    return \" [SEP] \".join(parts)\n",
    "\n",
    "print(\"[INFO] Building combined text column...\")\n",
    "df[\"combined_text\"] = df.apply(make_text, axis=1)\n",
    "texts = df[\"combined_text\"].fillna(\"\").tolist()\n",
    "\n",
    "print(\"[INFO] Example text sample:\\n\", texts[0][:500], \"...\\n\")\n",
    "\n",
    "# dataset class\n",
    "class JobPostingDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = list(labels)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# device + tokenizer\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "print(\"[INFO] Loading DistilBERT tokenizer...\")\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# train / eval helpers\n",
    "def train_one_epoch(model, train_loader, optimizer, device, epoch, num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    print(f\"\\n[TRAIN] Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if step % 50 == 0 or step == 1:\n",
    "            print(f\"  Step {step:4d} | Batch loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"[TRAIN] Epoch {epoch+1} finished. Avg loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def eval_on_loader(model, val_loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs  = []\n",
    "\n",
    "    print(\"\\n[EVAL] Running on validation set...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # labels on cpu for metrics\n",
    "            labels = batch[\"labels\"].numpy()\n",
    "            all_labels.append(labels)\n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "\n",
    "            all_probs.append(probs)\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_probs  = np.concatenate(all_probs)\n",
    "    preds = (all_probs >= 0.5).astype(int)\n",
    "\n",
    "    precision = precision_score(all_labels, preds)\n",
    "    recall    = recall_score(all_labels, preds)\n",
    "    f1        = f1_score(all_labels, preds)\n",
    "    auc       = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    print(f\"[EVAL] Precision: {precision:.5f}\")\n",
    "    print(f\"[EVAL] Recall:    {recall:.5f}\")\n",
    "    print(f\"[EVAL] F1:        {f1:.5f}\")\n",
    "    print(f\"[EVAL] ROC AUC:   {auc:.5f}\")\n",
    "\n",
    "    return precision, recall, f1, auc\n",
    "\n",
    "# 3-fold stratified cross-validation\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(texts, labels), start=1):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"[FOLD {fold}/{N_SPLITS}]\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # split texts and labels for this fold\n",
    "    X_train_texts = [texts[i] for i in train_idx]\n",
    "    X_val_texts   = [texts[i] for i in val_idx]\n",
    "    y_train       = labels[train_idx]\n",
    "    y_val         = labels[val_idx]\n",
    "\n",
    "    print(f\"[FOLD {fold}] Train size: {len(X_train_texts)} | Val size: {len(X_val_texts)}\")\n",
    "\n",
    "    # build datasets and loaders\n",
    "    train_dataset = JobPostingDataset(X_train_texts, y_train, tokenizer, max_length=MAX_LENGTH)\n",
    "    val_dataset   = JobPostingDataset(X_val_texts,   y_val,   tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # re-init model + optimizer for this fold\n",
    "    print(f\"[FOLD {fold}] Loading DistilBERT model...\")\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=2,\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    best_metrics = None\n",
    "\n",
    "    # train for epochs on this fold\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_one_epoch(model, train_loader, optimizer, device, epoch, EPOCHS)\n",
    "        precision, recall, f1, auc = eval_on_loader(model, val_loader, device)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            print(f\"[FOLD {fold}] New best F1: {f1:.5f} (prev {best_f1:.5f})\")\n",
    "            best_f1 = f1\n",
    "            best_metrics = (precision, recall, f1, auc)\n",
    "\n",
    "    print(f\"\\n[FOLD {fold}] Best metrics:\")\n",
    "    print(f\"  Precision: {best_metrics[0]:.5f}\")\n",
    "    print(f\"  Recall:    {best_metrics[1]:.5f}\")\n",
    "    print(f\"  F1:        {best_metrics[2]:.5f}\")\n",
    "    print(f\"  ROC AUC:   {best_metrics[3]:.5f}\")\n",
    "\n",
    "    fold_metrics.append(best_metrics)\n",
    "\n",
    "# summary across folds\n",
    "fold_metrics = np.array(fold_metrics)\n",
    "\n",
    "mean_precision = fold_metrics[:, 0].mean()\n",
    "mean_recall    = fold_metrics[:, 1].mean()\n",
    "mean_f1        = fold_metrics[:, 2].mean()\n",
    "mean_auc       = fold_metrics[:, 3].mean()\n",
    "\n",
    "std_precision  = fold_metrics[:, 0].std()\n",
    "std_recall     = fold_metrics[:, 1].std()\n",
    "std_f1         = fold_metrics[:, 2].std()\n",
    "std_auc        = fold_metrics[:, 3].std()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  3-FOLD CV SUMMARY (Best per fold, then mean ± std)\")\n",
    "print(\"=\" * 60)\n",
    "for i, (p, r, f1, auc) in enumerate(fold_metrics, start=1):\n",
    "    print(f\"Fold {i}: P={p:.5f}, R={r:.5f}, F1={f1:.5f}, AUC={auc:.5f}\")\n",
    "\n",
    "print(\"\\n[MEAN ± STD]\")\n",
    "print(f\"Precision: {mean_precision:.5f} ± {std_precision:.5f}\")\n",
    "print(f\"Recall:    {mean_recall:.5f} ± {std_recall:.5f}\")\n",
    "print(f\"F1:        {mean_f1:.5f} ± {std_f1:.5f}\")\n",
    "print(f\"ROC AUC:   {mean_auc:.5f} ± {std_auc:.5f}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f29b23",
   "metadata": {},
   "source": [
    "### Full Training\n",
    "\n",
    "We did this for BiLSTM and DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90fbf29",
   "metadata": {},
   "source": [
    "_BiLISTM_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4466308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train final bi-lstm model on full dataset with train/test split\n",
    "def train_final_model(X, y, test_size=0.2, epochs=15, patience=5, save_dir='models'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"FINAL MODEL TRAINING - BI-DIRECTIONAL LSTM\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # stratified train/test split\n",
    "    print(\"\\n[1/6] Splitting data...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        random_state=42, \n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Train samples: {len(X_train):,} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    print(f\"Test samples:  {len(X_test):,} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "    print(f\"Train fraud rate: {y_train.mean()*100:.2f}%\")\n",
    "    print(f\"Test fraud rate:  {y_test.mean()*100:.2f}%\")\n",
    "    \n",
    "    # build feature pipeline\n",
    "    print(\"\\n[2/6] Building feature pipeline on train set...\")\n",
    "    pipeline = build_complete_pipeline()\n",
    "    X_train_engineered = pipeline.fit_transform(X_train)\n",
    "    X_test_engineered = pipeline.transform(X_test)\n",
    "    \n",
    "    print(f\"Feature pipeline fitted!\")\n",
    "    print(f\"Engineered features shape: {X_train_engineered.shape}\")\n",
    "    \n",
    "    if hasattr(X_train_engineered, 'toarray'):\n",
    "        print(\"Converting sparse matrices to dense...\")\n",
    "        X_train_engineered = X_train_engineered.toarray()\n",
    "        X_test_engineered = X_test_engineered.toarray()\n",
    "    \n",
    "    # compute class weights\n",
    "    print(\"\\n[3/6] Computing class weights...\")\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    class_weight_dict = {i: w for i, w in enumerate(class_weights)}\n",
    "    print(f\"Class weights: {class_weight_dict}\")\n",
    "    \n",
    "    # prepare text data\n",
    "    print(\"\\n[4/6] Preparing text data...\")\n",
    "    \n",
    "    X_train_text = {\n",
    "        'title': np.array(X_train['title'].fillna('').astype(str).tolist(), dtype=object),\n",
    "        'company_profile': np.array(X_train['company_profile'].fillna('').astype(str).tolist(), dtype=object),\n",
    "        'description': np.array(X_train['description'].fillna('').astype(str).tolist(), dtype=object),\n",
    "        'requirements': np.array(X_train['requirements'].fillna('').astype(str).tolist(), dtype=object),\n",
    "        'benefits': np.array(X_train['benefits'].fillna('').astype(str).tolist(), dtype=object)\n",
    "    }\n",
    "    \n",
    "    X_test_text = {\n",
    "        'title': np.array(X_test['title'].fillna('').astype(str).tolist(), dtype=object),\n",
    "        'company_profile': np.array(X_test['company_profile'].fillna('').astype(str).tolist(), dtype=object),\n",
    "        'description': np.array(X_test['description'].fillna('').astype(str).tolist(), dtype=object),\n",
    "        'requirements': np.array(X_test['requirements'].fillna('').astype(str).tolist(), dtype=object),\n",
    "        'benefits': np.array(X_test['benefits'].fillna('').astype(str).tolist(), dtype=object)\n",
    "    }\n",
    "    \n",
    "    print(\"Text data prepared!\")\n",
    "    \n",
    "    # build and train model\n",
    "    print(\"\\n[5/6] Building hybrid Bi-LSTM model...\")\n",
    "    model, text_vectorizer = build_hybrid_model(\n",
    "        engineered_feature_dim=X_train_engineered.shape[1]\n",
    "    )\n",
    "    \n",
    "    print(\"Adapting text vectorizer to training data...\")\n",
    "    all_train_text = np.concatenate([\n",
    "        X_train['title'].fillna('').values,\n",
    "        X_train['company_profile'].fillna('').values,\n",
    "        X_train['description'].fillna('').values,\n",
    "        X_train['requirements'].fillna('').values,\n",
    "        X_train['benefits'].fillna('').values\n",
    "    ])\n",
    "    text_vectorizer.adapt(all_train_text)\n",
    "    \n",
    "    train_inputs = {\n",
    "        'engineered_features': X_train_engineered,\n",
    "        **X_train_text\n",
    "    }\n",
    "    \n",
    "    test_inputs = {\n",
    "        'engineered_features': X_test_engineered,\n",
    "        **X_test_text\n",
    "    }\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            patience=patience, \n",
    "            mode='max', \n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            patience=3, \n",
    "            factor=0.5,\n",
    "            min_lr=0.00001,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(save_dir, 'best_model.h5'),\n",
    "            monitor='val_auc',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nTraining model for up to {epochs} epochs...\")\n",
    "    print(f\"Early stopping patience: {patience} epochs\")\n",
    "    print(f\"Validation split: 10% of training data\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_inputs,\n",
    "        y_train,\n",
    "        validation_split=0.1,\n",
    "        class_weight=class_weight_dict,\n",
    "        epochs=epochs,\n",
    "        batch_size=32,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"\\n[6/6] Evaluating on test set...\")\n",
    "    \n",
    "    y_pred_proba = model.predict(test_inputs, verbose=0).flatten()\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "    \n",
    "    test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    test_precision = precision_score(y_test, y_pred)\n",
    "    test_recall = recall_score(y_test, y_pred)\n",
    "    test_f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred, target_names=['Legitimate', 'Fraudulent'])\n",
    "    \n",
    "    test_metrics = {\n",
    "        'auc': test_auc,\n",
    "        'precision': test_precision,\n",
    "        'recall': test_recall,\n",
    "        'f1': test_f1,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'classification_report': class_report\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL TEST SET EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Test AUC:       {test_auc:.5f}\")\n",
    "    print(f\"Test Precision: {test_precision:.5f}\")\n",
    "    print(f\"Test Recall:    {test_recall:.5f}\")\n",
    "    print(f\"Test F1:        {test_f1:.5f}\")\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(\"                 Predicted\")\n",
    "    print(\"              Legit   Fraud\")\n",
    "    print(f\"Actual Legit  {conf_matrix[0][0]:5d}   {conf_matrix[0][1]:5d}\")\n",
    "    print(f\"       Fraud  {conf_matrix[1][0]:5d}   {conf_matrix[1][1]:5d}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(class_report)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"TRAINING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # compare with CV result\n",
    "    cv_auc = 0.99184\n",
    "    auc_diff = test_auc - cv_auc\n",
    "    print(f\"\\nPerformance Comparison:\")\n",
    "    print(f\"   CV AUC:   {cv_auc:.5f}\")\n",
    "    print(f\"   Test AUC: {test_auc:.5f}\")\n",
    "    print(f\"   Difference: {auc_diff:+.5f}\")\n",
    "    \n",
    "    if test_auc >= 0.990:\n",
    "        print(\"\\nTest performance matches CV. Model is ready.\")\n",
    "    elif test_auc >= 0.985:\n",
    "        print(\"\\nTest performance is strong. Consider if further tuning is needed.\")\n",
    "    else:\n",
    "        print(\"\\nTest performance dropped significantly. Consider hyperparameter tuning.\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return model, pipeline, test_metrics, history, y_pred_proba, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e6db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL MODEL TRAINING - BI-DIRECTIONAL LSTM\n",
      "============================================================\n",
      "\n",
      "[1/6] Splitting data...\n",
      "Train samples: 14,304 (80.0%)\n",
      "Test samples:  3,576 (20.0%)\n",
      "Train fraud rate: 4.84%\n",
      "Test fraud rate:  4.84%\n",
      "\n",
      "[2/6] Building feature pipeline on train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0077cdd3ec4447f8ffc4bde910a0e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3024bbbc0a734654940ed1908d4c83ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c605b6fd2e543e49c25824f12f1503d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e492c5cbe24f668de4a803d7525556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94bf8c0badcd427497bf4e9d07d421eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaba27bfbb2c4ccb9093ca53d8df30b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63fe01e2d974b0fb9580c7e5e407f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c324e1bb986946eeafeb90f0b6ebaa05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53a1c42c2d5457fa3a0c9bef717e435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5004d8356b7445f9fbafb9d8a3aea79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933738d063664f70827efde6a35348e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using cuda for sentence transformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27c5a8a69ed46df8234d72d4b45118b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/224 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f953516b168445469397f1eaae95d39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/224 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10ed8e023b24e9080460d2f39681983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd14e0eab8547ada5de86af5ebb2817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature pipeline fitted!\n",
      "Engineered features shape: (14304, 5187)\n",
      "Converting sparse matrices to dense...\n",
      "\n",
      "[3/6] Computing class weights...\n",
      "Class weights: {0: 0.5254573506722504, 1: 10.32034632034632}\n",
      "\n",
      "[4/6] Preparing text data...\n",
      "Text data prepared!\n",
      "\n",
      "[5/6] Building hybrid Bi-LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1763375839.097009      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12944 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1763375839.097610      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapting text vectorizer to training data...\n",
      "\n",
      "Training model for up to 15 epochs...\n",
      "Early stopping patience: 5 epochs\n",
      "Validation split: 10% of training data\n",
      "============================================================\n",
      "Epoch 1/15\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.8863 - auc: 0.8724 - loss: 0.4083 - precision: 0.2412 - recall: 0.6763\n",
      "Epoch 1: val_auc improved from -inf to 0.98039, saving model to /kaggle/working/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2084s\u001b[0m 5s/step - accuracy: 0.8863 - auc: 0.8726 - loss: 0.4080 - precision: 0.2414 - recall: 0.6767 - val_accuracy: 0.9154 - val_auc: 0.9804 - val_loss: 0.1714 - val_precision: 0.3118 - val_recall: 0.9298 - learning_rate: 0.0010\n",
      "Epoch 2/15\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.9663 - auc: 0.9956 - loss: 0.0831 - precision: 0.6039 - recall: 0.9635\n",
      "Epoch 2: val_auc did not improve from 0.98039\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2020s\u001b[0m 5s/step - accuracy: 0.9663 - auc: 0.9955 - loss: 0.0832 - precision: 0.6038 - recall: 0.9635 - val_accuracy: 0.9448 - val_auc: 0.9632 - val_loss: 0.1247 - val_precision: 0.4127 - val_recall: 0.9123 - learning_rate: 0.0010\n",
      "Epoch 3/15\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.9768 - auc: 0.9965 - loss: 0.0556 - precision: 0.6839 - recall: 0.9893\n",
      "Epoch 3: val_auc did not improve from 0.98039\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2009s\u001b[0m 5s/step - accuracy: 0.9768 - auc: 0.9965 - loss: 0.0556 - precision: 0.6840 - recall: 0.9893 - val_accuracy: 0.9860 - val_auc: 0.9526 - val_loss: 0.0714 - val_precision: 0.8246 - val_recall: 0.8246 - learning_rate: 0.0010\n",
      "Epoch 4/15\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.9929 - auc: 0.9965 - loss: 0.0209 - precision: 0.8753 - recall: 0.9930\n",
      "Epoch 4: val_auc did not improve from 0.98039\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2036s\u001b[0m 5s/step - accuracy: 0.9928 - auc: 0.9965 - loss: 0.0209 - precision: 0.8752 - recall: 0.9930 - val_accuracy: 0.9860 - val_auc: 0.9093 - val_loss: 0.0985 - val_precision: 0.8936 - val_recall: 0.7368 - learning_rate: 0.0010\n",
      "Epoch 5/15\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.9883 - auc: 0.9988 - loss: 0.0304 - precision: 0.8128 - recall: 0.9914\n",
      "Epoch 5: val_auc did not improve from 0.98039\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2135s\u001b[0m 5s/step - accuracy: 0.9883 - auc: 0.9988 - loss: 0.0304 - precision: 0.8127 - recall: 0.9914 - val_accuracy: 0.9846 - val_auc: 0.9703 - val_loss: 0.0481 - val_precision: 0.7778 - val_recall: 0.8596 - learning_rate: 0.0010\n",
      "Epoch 6/15\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.9940 - auc: 0.9999 - loss: 0.0146 - precision: 0.8992 - recall: 0.9963\n",
      "Epoch 6: val_auc did not improve from 0.98039\n",
      "\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2074s\u001b[0m 5s/step - accuracy: 0.9940 - auc: 0.9999 - loss: 0.0147 - precision: 0.8990 - recall: 0.9963 - val_accuracy: 0.9720 - val_auc: 0.9491 - val_loss: 0.1323 - val_precision: 0.6000 - val_recall: 0.8947 - learning_rate: 0.0010\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "[6/6] Evaluating on test set...\n",
      "\n",
      "============================================================\n",
      "FINAL TEST SET EVALUATION\n",
      "============================================================\n",
      "Test AUC:       0.98771\n",
      "Test Precision: 0.34874\n",
      "Test Recall:    0.95954\n",
      "Test F1:        0.51156\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Predicted\n",
      "              Legit   Fraud\n",
      "Actual Legit   3093     310\n",
      "       Fraud      7     166\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Legitimate       1.00      0.91      0.95      3403\n",
      "  Fraudulent       0.35      0.96      0.51       173\n",
      "\n",
      "    accuracy                           0.91      3576\n",
      "   macro avg       0.67      0.93      0.73      3576\n",
      "weighted avg       0.97      0.91      0.93      3576\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n",
      "\n",
      "📊 Performance Comparison:\n",
      "   CV AUC:   0.99184\n",
      "   Test AUC: 0.98771\n",
      "   Difference: -0.00413\n",
      "\n",
      "Test performance is strong. Consider if further tuning is needed.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "model, pipeline, test_metrics, history, y_pred_proba, y_test = train_final_model(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    epochs=15, # max 15 epochs\n",
    "    patience=5, # stop if no improvement for 5 epochs\n",
    "    save_dir='models'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f43cb4",
   "metadata": {},
   "source": [
    "_DistilBERT_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90a2f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading CSV...\n",
      "[INFO] Data loaded: (17880, 18)\n",
      "[INFO] Building combined text column...\n",
      "[INFO] Example text sample:\n",
      " Marketing Intern [SEP] US, NY, New York [SEP] Marketing [SEP] Other [SEP] Internship [SEP] nan [SEP] nan [SEP] Marketing [SEP] We're Food52, and we've created a groundbreaking and award-winning cooking site. We support, connect, and celebrate home cooks, and give them everything they need in one place.We have a top editorial, business, and engineering team. We're focused on using technology to find new and better ways to connect people around their specific food interests, and to offer them supe ...\n",
      "\n",
      "[INFO] Splitting train/validation...\n",
      "[INFO] Train size: 14304\n",
      "[INFO] Val size:   3576\n",
      "[INFO] Loading DistilBERT tokenizer...\n",
      "[INFO] Using device: mps\n",
      "[INFO] Loading DistilBERT classification head...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TRAIN] Epoch 1/3\n",
      "  Step    1 | Batch loss: 0.6872\n",
      "  Step   50 | Batch loss: 0.2436\n",
      "  Step  100 | Batch loss: 0.1258\n",
      "  Step  150 | Batch loss: 0.0801\n",
      "  Step  200 | Batch loss: 0.1151\n",
      "  Step  250 | Batch loss: 0.1016\n",
      "  Step  300 | Batch loss: 0.0682\n",
      "  Step  350 | Batch loss: 0.2627\n",
      "  Step  400 | Batch loss: 0.0625\n",
      "  Step  450 | Batch loss: 0.0052\n",
      "  Step  500 | Batch loss: 0.0358\n",
      "  Step  550 | Batch loss: 0.0273\n",
      "  Step  600 | Batch loss: 0.0193\n",
      "  Step  650 | Batch loss: 0.0165\n",
      "  Step  700 | Batch loss: 0.0263\n",
      "  Step  750 | Batch loss: 0.0132\n",
      "  Step  800 | Batch loss: 0.0122\n",
      "  Step  850 | Batch loss: 0.0047\n",
      "[TRAIN] Epoch 1 finished. Avg loss: 0.1120\n",
      "\n",
      "[EVAL] Running on validation set...\n",
      "[EVAL] Precision: 0.89542\n",
      "[EVAL] Recall:    0.79191\n",
      "[EVAL] F1:        0.84049\n",
      "[EVAL] ROC AUC:   0.98648\n",
      "[MODEL] New best F1: 0.84049 (prev 0.00000). Saving weights in memory.\n",
      "\n",
      "[TRAIN] Epoch 2/3\n",
      "  Step    1 | Batch loss: 0.0062\n",
      "  Step   50 | Batch loss: 0.0248\n",
      "  Step  100 | Batch loss: 0.0072\n",
      "  Step  150 | Batch loss: 0.0045\n",
      "  Step  200 | Batch loss: 0.0037\n",
      "  Step  250 | Batch loss: 0.0480\n",
      "  Step  300 | Batch loss: 0.0039\n",
      "  Step  350 | Batch loss: 0.0227\n",
      "  Step  400 | Batch loss: 0.1670\n",
      "  Step  450 | Batch loss: 0.0042\n",
      "  Step  500 | Batch loss: 0.1725\n",
      "  Step  550 | Batch loss: 0.0025\n",
      "  Step  600 | Batch loss: 0.1926\n",
      "  Step  650 | Batch loss: 0.0724\n",
      "  Step  700 | Batch loss: 0.0736\n",
      "  Step  750 | Batch loss: 0.0056\n",
      "  Step  800 | Batch loss: 0.0137\n",
      "  Step  850 | Batch loss: 0.0048\n",
      "[TRAIN] Epoch 2 finished. Avg loss: 0.0457\n",
      "\n",
      "[EVAL] Running on validation set...\n",
      "[EVAL] Precision: 0.90506\n",
      "[EVAL] Recall:    0.82659\n",
      "[EVAL] F1:        0.86405\n",
      "[EVAL] ROC AUC:   0.99159\n",
      "[MODEL] New best F1: 0.86405 (prev 0.84049). Saving weights in memory.\n",
      "\n",
      "[TRAIN] Epoch 3/3\n",
      "  Step    1 | Batch loss: 0.0041\n",
      "  Step   50 | Batch loss: 0.0042\n",
      "  Step  100 | Batch loss: 0.0037\n",
      "  Step  150 | Batch loss: 0.0029\n",
      "  Step  200 | Batch loss: 0.0543\n",
      "  Step  250 | Batch loss: 0.0032\n",
      "  Step  300 | Batch loss: 0.0017\n",
      "  Step  350 | Batch loss: 0.0047\n",
      "  Step  400 | Batch loss: 0.0062\n",
      "  Step  450 | Batch loss: 0.0035\n",
      "  Step  500 | Batch loss: 0.0014\n",
      "  Step  550 | Batch loss: 0.1987\n",
      "  Step  600 | Batch loss: 0.0015\n",
      "  Step  650 | Batch loss: 0.0123\n",
      "  Step  700 | Batch loss: 0.0132\n",
      "  Step  750 | Batch loss: 0.0115\n",
      "  Step  800 | Batch loss: 0.0020\n",
      "  Step  850 | Batch loss: 0.0090\n",
      "[TRAIN] Epoch 3 finished. Avg loss: 0.0247\n",
      "\n",
      "[EVAL] Running on validation set...\n",
      "[EVAL] Precision: 0.93711\n",
      "[EVAL] Recall:    0.86127\n",
      "[EVAL] F1:        0.89759\n",
      "[EVAL] ROC AUC:   0.99364\n",
      "[MODEL] New best F1: 0.89759 (prev 0.86405). Saving weights in memory.\n",
      "\n",
      "======================================\n",
      "  TRAINING COMPLETE\n",
      "  Best validation F1: 0.8975903614457831\n",
      "======================================\n",
      "[SAVE] Saving best model to: distilbert_fakejobs_finetuned\n",
      "[SAVE] Done.\n"
     ]
    }
   ],
   "source": [
    "# target\n",
    "labels = df[\"fraudulent\"].astype(int).values\n",
    "\n",
    "# build text input - one big text per row\n",
    "def make_text(row):\n",
    "    parts = [\n",
    "        str(row.get(\"title\", \"\")),\n",
    "        str(row.get(\"location\", \"\")),\n",
    "        str(row.get(\"department\", \"\")),\n",
    "        str(row.get(\"employment_type\", \"\")),\n",
    "        str(row.get(\"required_experience\", \"\")),\n",
    "        str(row.get(\"required_education\", \"\")),\n",
    "        str(row.get(\"industry\", \"\")),\n",
    "        str(row.get(\"function\", \"\")),\n",
    "        str(row.get(\"company_profile\", \"\")),\n",
    "        str(row.get(\"description\", \"\")),\n",
    "        str(row.get(\"requirements\", \"\")),\n",
    "        str(row.get(\"benefits\", \"\")),\n",
    "    ]\n",
    "    # join with separators so bert gets some structure\n",
    "    return \" [SEP] \".join(parts)\n",
    "\n",
    "print(\"[INFO] Building combined text column...\")\n",
    "df[\"combined_text\"] = df.apply(make_text, axis=1)\n",
    "\n",
    "texts = df[\"combined_text\"].fillna(\"\").tolist()\n",
    "print(\"[INFO] Example text sample:\\n\", texts[0][:500], \"...\\n\")\n",
    "\n",
    "# train-val split\n",
    "print(\"[INFO] Splitting train/validation...\")\n",
    "X_train_texts, X_val_texts, y_train, y_val = train_test_split(\n",
    "    texts,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels,\n",
    ")\n",
    "\n",
    "print(\"[INFO] Train size:\", len(X_train_texts))\n",
    "print(\"[INFO] Val size:  \", len(X_val_texts))\n",
    "\n",
    "# dataset + dataloader\n",
    "class JobPostingDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = list(labels)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # squeeze to remove batch dimension\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "\n",
    "print(\"[INFO] Loading DistilBERT tokenizer...\")\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "train_dataset = JobPostingDataset(X_train_texts, y_train, tokenizer, max_length=256)\n",
    "val_dataset   = JobPostingDataset(X_val_texts,   y_val,   tokenizer, max_length=256)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# model + device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "print(\"[INFO] Loading DistilBERT classification head...\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# training + evaluation loops\n",
    "EPOCHS = 3\n",
    "\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    print(f\"\\n[TRAIN] Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if step % 50 == 0 or step == 1:\n",
    "            print(f\"  Step {step:4d} | Batch loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"[TRAIN] Epoch {epoch+1} finished. Avg loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def eval_on_val():\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs  = []\n",
    "\n",
    "    print(\"\\n[EVAL] Running on validation set...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            labels = batch[\"labels\"].numpy()\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "\n",
    "            all_labels.append(labels)\n",
    "            all_probs.append(probs)\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_probs  = np.concatenate(all_probs)\n",
    "    preds = (all_probs >= 0.5).astype(int)\n",
    "\n",
    "    precision = precision_score(all_labels, preds)\n",
    "    recall    = recall_score(all_labels, preds)\n",
    "    f1        = f1_score(all_labels, preds)\n",
    "    auc       = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    print(f\"[EVAL] Precision: {precision:.5f}\")\n",
    "    print(f\"[EVAL] Recall:    {recall:.5f}\")\n",
    "    print(f\"[EVAL] F1:        {f1:.5f}\")\n",
    "    print(f\"[EVAL] ROC AUC:   {auc:.5f}\")\n",
    "\n",
    "    return precision, recall, f1, auc\n",
    "\n",
    "\n",
    "# full training run\n",
    "best_f1 = 0.0\n",
    "best_state_dict = None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_one_epoch(epoch)\n",
    "    precision, recall, f1, auc = eval_on_val()\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        print(f\"[MODEL] New best F1: {f1:.5f} (prev {best_f1:.5f}). Saving weights in memory.\")\n",
    "        best_f1 = f1\n",
    "        best_state_dict = model.state_dict()\n",
    "\n",
    "print(\"\\n======================================\")\n",
    "print(\"  TRAINING COMPLETE\")\n",
    "print(\"  Best validation F1:\", f1)\n",
    "print(\"======================================\")\n",
    "\n",
    "if best_state_dict is not None:\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    save_path = \"distilbert_fakejobs_finetuned\"\n",
    "    print(f\"[SAVE] Saving best model to: {save_path}\")\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    print(\"[SAVE] Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe28442",
   "metadata": {},
   "source": [
    "BiLSTM has a high recall, while pre-trained DistilBERT has a high precision; both models can be used as final models depending on use case. Please refer to our report for more discussions on this."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
